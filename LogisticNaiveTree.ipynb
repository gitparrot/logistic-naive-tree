{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "hw2_template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsXUGXOlvBKu"
      },
      "source": [
        "# CS 613 - Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB618jMGvBK3"
      },
      "source": [
        "# Assignment 2 - Classification\n",
        "\n",
        "Name <br>\n",
        "CS 613 Machine Learning <br>\n",
        "Fall 2021 <br>\n",
        "Dr. Edward Kim <br>\n",
        "Drexel University <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuKZQtzJvBK5"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this assignment you will perform classification using Logistic Regression, Naive Bayes and Decision Tree classifiers.  You will run your implementations on  a binary class dataset and report your results.\n",
        "\n",
        "You may __NOT__ use any functions from a ML library in your code unless explicitly told otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ahea5xwvBK6"
      },
      "source": [
        "## Submission\n",
        "For your submission, upload to Blackboard a single zip file containing:\n",
        "1.  PDF Writeup and PDF of Jupyter Notebook (can be the same PDF)\n",
        "2.  Python notebook Code\n",
        "\n",
        "The PDF document should contain the following at the top:\n",
        "\n",
        "__1. Answers to Theory Questions__\n",
        "\n",
        "_1.1_\n",
        "- a. What is the sample entropy, ùêª(ùëå) from this training data (using log base 2) (2pts)?\n",
        "- b. What are the information gains for branching on variables $x_1$ and $x_2$ (2pts)? <br>\n",
        "- c. Draw the decision tree that would be learned by the ID3 algorithm without pruning from this training data (3pts)?\n",
        "\n",
        "\n",
        "_1.2_\n",
        "- a. What are the class priors, $P(A=Yes), P(A=No)$? (2pt)\n",
        "- b. Find the parameters of the Gaussians necessary to do Gaussian Naive Bayes classification on this decision to give an A or not.  Standardize the features first over all the data together so that there is no unfair bias towards the features of different scales (2pts).\n",
        "- c. Using your response from the prior question, determine if an essay with 242 characters and an average word length of 4.56 should get an A or not (3pts).\n",
        "\n",
        "\n",
        "_1.3_\n",
        "- a. How could you use a validation set to determine the user-defined parameter $k$?\n",
        "\n",
        "__2. Requested Logistic Regression thetas and plots__\n",
        "\n",
        "__3. Requested Classification Statistics__\n",
        "> precision: <br>\n",
        "> recall: <br>\n",
        "> f1_score: <br>\n",
        "> accuracy: <br>\n",
        "\n",
        "\n",
        "__4. Requested Classification Statistics__\n",
        "> precision: <br>\n",
        "> recall: <br>\n",
        "> f1_score: <br>\n",
        "> accuracy: \n",
        "\n",
        "__5. Requested Classification Statistics__\n",
        "> precision: <br>\n",
        "> recall: <br>\n",
        "> f1_score: <br>\n",
        "> accuracy: <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4nsi0ZkvBK8"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agRa12HrvBK9"
      },
      "source": [
        "### Iris Dataset  (sklearn.datasets.load_iris)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHoHuPUgvBK-"
      },
      "source": [
        "The Iris flower data set or Fishers Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.\n",
        "\n",
        "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
        "\n",
        "The iris data set is widely used as a beginner's dataset for machine learning purposes. The dataset is included in the machine learning package Scikit-learn, so that users can access it without having to find a source for it. The following python code illustrates usage.\n",
        "\n",
        "```\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLcKPwekvBLA"
      },
      "source": [
        "### Spambase Dataset  (spambase.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmyKV43RvBLB"
      },
      "source": [
        "This dataset consists of 4601 instances of data, each with 57 features and a class label designating if the sample is spam or not.\n",
        "The features are _real valued_ and are described in much detail here:\n",
        "\n",
        "> https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\n",
        "\n",
        "Data obtained from:  https://archive.ics.uci.edu/ml/datasets/Spambase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic52UxFMvBLD"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBACzyD3vBLF"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.datasets import load_iris \n",
        "import random as rand\n",
        "import requests as req\n",
        "from numpy import log as ln\n",
        "from math import exp\n",
        "from math import sqrt\n",
        "from math import pi\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "\n",
        "import matplotlib as mpl\n",
        "\n",
        "from pprint import pprint\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from scipy.special import logsumexp"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3R3S9ILvBLJ"
      },
      "source": [
        "## 1 Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1rhNn4SvBLK"
      },
      "source": [
        "__1__ Consider the following set of training examples for an unknown target function:  $(x_1, x_2)\\rightarrow y$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY2w9pzQvBLK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "768dbfeb-8c6a-4b3b-d34b-0cf7df5c9900"
      },
      "source": [
        "theory_data1 = pd.DataFrame({\n",
        "   \"Y\" : ['+', '+', '+', '+', '-', '-', '-', '-'], \n",
        "    \"x1\" : ['T', 'T', 'F', 'F', 'T', 'T', 'F', 'F'],\n",
        "    \"x2\" : ['T', 'F', 'T', 'F', 'T', 'F', 'T', 'F'],\n",
        "    \"Count\" : [3, 4, 4, 1, 0, 1, 3, 5]\n",
        "})\n",
        "theory_data1['Y'] = abs(theory_data1['Y'].astype('category').cat.codes - 1)\n",
        "theory_data1"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Y</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>T</td>\n",
              "      <td>T</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>T</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>F</td>\n",
              "      <td>T</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Y x1 x2  Count\n",
              "0  1  T  T      3\n",
              "1  1  T  F      4\n",
              "2  1  F  T      4\n",
              "3  1  F  F      1\n",
              "4  0  T  T      0\n",
              "5  0  T  F      1\n",
              "6  0  F  T      3\n",
              "7  0  F  F      5"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z59LndJMvBLM"
      },
      "source": [
        "a. What is the sample entropy, $H(Y)$ from this training data (using log base 2) (2pts)? <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUzpTFxRdHly"
      },
      "source": [
        "H(Y) = -(12/21log2(12/21) + 9/21log2(9/21)) = 0.985\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfvrb9nQvBLN"
      },
      "source": [
        "b. item What are the information gains for branching on variables $x_1$ and $x_2$ (2pts)? <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85pRQEHYeKnP"
      },
      "source": [
        "E(x1) = 8/21(-7/8log2(7/8) - 1/8log2(1/8)) + 13/21(-5/13log2(5/13) - 8/13LOG2(8/13) = 0.802"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssqd0ZMffODm"
      },
      "source": [
        "IG(x1) = 0.985 - 0.802 = 0.183"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5mTHQ98fQZs"
      },
      "source": [
        "E(x2) = 10/21(-7/10log2(7/10) - 3/10log2(3/10)) + 11/21(-5/11log2(5/11) - 6/11log2(6/11)) = 0.939"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTejOmY5f387"
      },
      "source": [
        "IG(x2) = 0.985 - 0.939 = 0.046"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W1GRAn9vBLO"
      },
      "source": [
        "c. item Draw the deicion tree that would be learned by the ID3 algorithm without pruning from this training data (3pts)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1OCWCztvBLP"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCmYt3bmf3BG"
      },
      "source": [
        "![MLHW2.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABgAAAANgCAIAAABlQDTBAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAFDMSURBVHhe7d1tciM30qjRd11ekNfj1XgzXsy97e4HbtRQlCiJVayPc/54nAmyABQYkcjo9vzf/wMAAADg1DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAAAAAE5OAwgAAADg5DSAAK7u/76hrwAAAPZN7Q5wFfVsNtEjAQCAfVCjA5xTnZjdaFoAAMArqMgBzqNey+41XQAAYCuqcIDDq61yTK0BAABYk8ob4KjqoHxbX/clfcUz9I0AAMAKFNwAB1O/5PP6/CZ65Of1eQAA4KmU2gDHUIPkM/rkDjShz+iTAADAM6iwAfaujshj+syONdHH9BkAAOB71NYAO1UL5DF95mia/QP6AAAA8CVKaoA9qu3xkUYfX+t5V0MBAIDPU08D7Evdjnc19Ixa4X2NAwAAPkMlDbAXdTjua9wFtOD7GgcAADxGDQ3wenU17mvcxbT4+xoHAAB8RPUM8GI1M+5o0IW1EXc0CAAAeJfSGeBl6mHc0SB+alPuaBAAAHCHohngNWpdvKUR3GiD3tIIAADgLSpmgBeoaXGjNO9qs26UBgAAbiiXAbZWu+JGaR7Qlt0oDQAALKmVATZVo2KpHJ/U9i2VAwAAJgplgO3Uolgqx5e0iUvlAACAQZUMsJGaE0vl+Ia2cqkcAADwkxIZYAu1JZbK8W1t6FI5AABAAwhgAzUklsrxJG3rUjkAALg8xTHAumpFLJXjqdrcpXIAAHBtKmOAFdWEWCrHCtripXIAAHBhymKAtdR+WCrHatropXIAAHBVamKAtdR7mJRgZW33pAQAAFyVmhhgFTUeJiXYRJs+KQEAAJekIAZYRV2HoSgbausnJQAA4HpUwwDPV79hUoINtfWTEgAAcD2qYYDnq98wFGVzvYChKAAAXI9qGODJajZMShzPP3//+ce/C/jz7wJH83P7F0oAAMDFKIUBnqxOw1D0cEbz51+HbQD90BKGogAAcDFKYYBnqs0wKXEo//z1q/fzx59//vnvP0/UAPqhBAAAXIk6GOCZ6jEMRQ/m7z//7f389U//89gNoB9+vorfigIAwJWogwGeqR7DUHRX+uM9y6bOstHzzz//Nn/+pQEEAACnoA4GeKZ6DEPRnfnV1ZnaOvfbPBpAAABwCupggKepwTApsTuLFtB7TZ4zNoB+KAEAAJehCAZ4mroLQ9F9+tXZ+eOvv99v8ZyiAfTDv6uYFAUAgMtQBAM8Td2Foehe/ert/HS/waMBBAAAp6AIBniaugtD0d36+89f/2fv7/V3NIAAAOAUFMEAT1N3YSi6U/2fgf1yt8OjAQQAAKegCAZ4mroLQ9Fd+tX++eOvf2rx/Pu/3qIBBAAAp6AIBniaugtD0R3qT//U13mvBaQBBAAAp6AIBniaugtD0d1Ztn/+9b8toL//+jN//Bz6xx/9619H7QT9u4pJUQAAuAxFMMDT1F0Yiu7MP2/9oZ7/ekI/W0CL/zzQwr2/KbZ7zX8oCgAAl6EIBniauguTErxa72MoCgAAl6EIBnimGgxDUV6qlzEUBQCAK1EHAzxTPYahKC/VyxiKAgDAlaiDAZ6pHsNQlJfqZQxFAQDgStTBAM9Uj2FSghfpNUxKAADAlaiDAZ6sNsNQlBfpNQxFAQDgYpTCAE9Wp2FSgs31AiYlAADgYpTCAM9Xs2EoyuZ6AUNRAAC4HtUwwPPVb5iUYENt/aQEAABcj2oYYBW1HIaibKitn5QAAIDrUQ0DrKKWw6QEm2jTJyUAAOCSFMQAa6nxMCnBytruSQkAALgqNTHAWuo9LJVjNW30UjkAALgqNTHAimo/TEqwmjZ6UgIAAC5MWQywrpoQkxKsoC2elAAAgGtTGQOsrlbEUjmep52dlAAAgMtTHANsoYbEUjmeoT1dKgcAAJenOAbYQg2JG6X5hrbyRmkAAEADCGAztSVulOZL2sQbpQEAgJ+UyACbqj9xozSf0d7dKA0AAAyqZICt1aW4UZoHtGU3SgMAAEtqZYAXqF1xozTvarNulAYAAG4olwFeo6bFWxrBjTboLY0AAADeomIGeIGaFu9qKD+1KXc0CAAAuEPRDLC1mhaP6TMX1kbc0SAAAOBdSmeATdW3+KQ+fDEt/r7GAQAAH1E9A2ynvsVSOf2OSQu+r3EAAMBj1NAA26l7MSkxKXFf406qRd7XOAAA4DNU0gAbqYExKXGj9AP6wPG1no80GgAA+CTFNMAWamBMStzXuMf0maNp9p/RJwEAgM9QSQOsrtbFUrkH9IHH9Jl9a64PeHPwry8BAAAep4wGWF19i0mJz+iTn9End6AJfUaf1AACAIBnUEYDrKumxaTEV/Utn9fnN9Ejv6SvmJSYlAAAAB6jhgZYUe2KSYln6Bu/p+/6hr7o2/q6Oxo0KQEAADxAAQ2wlhoVS+Weqq8+ptbwkUZPSgAAAA9QQAOsoi7FUrnV9Jjda7qf1IcnJQAAgI+ongFWUYtiUmIrPXU3mtY39EWTEgAAwEdUzwDPV39iUuJ1mseGevBT9dWTEgAAwLuUzgBPVmdiUmKXmuI39EVb6amTEgAAwH3qZoBnqiexVI5naE8nJQAAgPvUzQDPVE9iUoLnaWcnJQAAgDsUzQBPUzdiUoJna38nJQAAgLeomAGeoz7EpAQraIsnJQAAgLeomAGeoCbEUjnW0S5PSgAAADeUywDfVfthqRyraaMnJQAAgBvKZYDvqv0wKcHK2u5JCQAAYEmtDPAtNR4mJdhEmz4pAQAATBTKAF9Xy2FSgq2075MSAADARKEM8EX1G5bKsaG2flICAAAYVMkAX1SzYVKCzfUChqIAAMCgSgb4ijoNkxK8Qu9gUgIAAPhJiQzwafUYJiV4nd7EpAQAAKABBPBZdReWyvE6vYlJCQAAQAMI4FNqLSyV49V6H5MSAABweYpjgE+orzApwT70ViYlAADg2lTGAI+qozApwW70YiYlAADg2lTGAA+pnTApwc70eiYlAADgwpTFAB+rkbBUjv3pDQ1FAQDgwpTFAB+oi7BUjl3qJU1KAADAVamJAT5QC2FSgh3rVU1KAADAJSmIAd5T82BSgn3rbU1KAADAJSmIAe6qczApwRH0ziYlAADgelTDAG+rZ7BUjoPotU1KAADAxSiFAd5Ww2BSguPozU1KAADAxSiFAd5Qt2BSgqPp/U1KAADAlaiDAf5XfYJJCQ6oVzgpAQAAV6IOBlioSbBUjmPqLU5KAADAZSiCAX6rPbBUjiPrXU5KAADANaiAAX6rNzApwcH1OiclAADgGlTAAKkxMCnBKfRSJyUAAOAClL8A/6olMCnBifRqh6IAAHAByl+AN7o/P5TjRHq1kxIAAHB2al8AfYEL6QVPSgAAwKkpfIGrqw0wKcEZ9Y4nJQAA4NQUvsCl1QOYlOC8etOTEgAAcF6qXuC6uv0vlePUetmTEgAAcFJKXuCiuvcvlePset+TEgAAcFJKXuCiuvdPSnANvfVJCQAAOCP1LnBF3fgnJbiS3v1QFAAAzki9C1xO1/1JCS6m1z8pAQAAp6PYBa6li/5SOa6nEzApAQAA56LSBa6lW/6kBJfUIZiUAACAc1HpAhfSFX9SggvrKExKAADAiShzgavocj8pweV1ICYlAADgLNS4wCV0rV8qx+V1ICYlAADgLNS4wPl1p18qBz91LCYlAADgFBS4wPl1oZ+UgEmHYygKAACnoMAFTq7b/KQELHU+JiUAAOD4VLfAmXWPn5SAt3RKJiUAAODglLbAaXWDXyoHb+mUTEoAAMDBKW2B0+oGPykB93VWJiUAAODI1LXAOXV3n5SAj3RiJiUAAOCwFLXACXVrn5SAB3RoJiUAAOCwFLXA2XRln5SAh3V0JiUAAOCYVLTAqXRZXyoHD+voTEoAAMAxqWiBU+myPikBn9QBmpQAAIADUs4C59E1fVICvqRjNCkBAABHo5YFTqIL+qQEfFUnaVICAACORi0LnEG386Vy8A0dpkkJAAA4FIUscHjdy5fKwbd1pIaiAABwKApZ4PC6l09KwDN0qiYlAADgOFSxwLF1I5+UgOfpbE1KAADAQShhgQPrLj4pAU/V8ZqUAACAg1DCAkfVRXypHDxbJ2xSAgAAjkD9ChxVt/BJCVhH52xSAgAAdk/xChxS9+9JCVhNR21SAgAAdk/xChxPl+9JCVhZB25SAgAA9k3lChxM1+6lcrC+ztxQFAAA9k3lChxJd+6lcrCJjt2kBAAA7JiyFTiSLtyTErChDt+kBAAA7JWaFTiMrtqTErCtzt+kBAAA7JWaFTiG7tmTEvAKncJJCQAA2CUFK3AA3bCXysGLdBAnJQAAYH9Uq8ABdL2elIDX6SxOSgAAwP6oVoG96249KQGv1omclAAAgJ1RqgK71q16UgJ2oEM5KQEAADujVAX2qyv1UjnYh87lpAQAAOyJOhXYqS7TS+VgTzqdkxIAALAbilRgp7pJT0rAznRAJyUAAGA3FKnAHnWNnpSAXeqYTkoAAMA+qFCB3ekCPSkBO9ZhnZQAAIAdUJ4C+9LVeakc7FiHdVICAAB2QHkK7EtX50kJ2L2O7KQEAAC8mtoUWEXX33c1dFJiUgKOoFM7KQEAAK+mNgWeoMvus/XtcByd3UkJAAB4KYUp8BVdbVfWw+BQOr6TEgAA8DqqUuBzutFuqAfDQXRwJyUAAOB1VKXAQ7rIvlRTgd3ryE5KAADAiyhJgQ90f/2MPvmAPvAZfRL2rfM6FAUAgBdRkgJ3dXN9QB/4tr7uAX0A9qqTOikBAACvoB4F3tad9b7GrabH3Nc42KtO6qTEUPQxfQYAAL5EQQn8r66b9zVuEz3yvsbB/nRG19STAADgI2pHYKFr5Vsa8SJN4i2NgD3pdG6rZwMAwA3FIpBukG9pxA40obc0Al6q4/hqzQYAAAY1IvCvbo1vacRuNK23NAI21xHcn+YHAMDlKQ2Bu3fX0rvUFG+Uhq108r6kr3hMn/mSvgIAgAtTFMLVdUG8UXrHmuiN0rCyDtyX9BXf1tc9ps8AAHBJykG4tO6FN0rvXtO9URrW0Tn7SKN/KjQUfaq++iONBgDgYhSCcGndCCclDqWpT0rAs3XC3tXQG6XXP5895l0NBQDgMpSAcF1dBCclDqgFTErA83S27mjQnjSz+xoHAMAFKP7gorr/TUocVsuYlIBv60jd0aC9apZ3NAgAgLNT+cEVdfNbKndYLWOpHHxDh+ktjTiCZvyWRgAAcGrKPriirn2TEgfXYiYl4Es6Rm9pxNE0+7c0AgCAk1LwwRV14RuKnkJLmpSAT+oAvaURh9UybpQGAOCMVHtwOV31JiVOoSVNSsBndHpulD6+1nOjNAAAp6PUg8vpnjcUPZEWNhSFh3V0bpQ+i1Z1ozQAAOeizoNr6YY3KfE9f/3Rt93zx1//NHR9PXJSAh7TuZmUOKNWOCkBAMC5qPPgWrrhDUW/7e8//5j05f3bT3/+vV0D6IdmMBSFB3RoJiXOq3VOSgAAcCKKPLiWrndD0ef65+efB9ryz/zc+LW6/xSFj3RilsqdWkudlAAA4CxUeHAhXewmJZ5rfw2gH0rAuzoukxJn12onJQAAOAsVHlxIF7uh6NPtoAH0w681/qco3NdZmZS4htY8KQEAwCko7+BCutUNRZ9OA4hj6qwMRdf195897X/8+XcDNtXDh6IAAJyC8g4upFvdUPTpNIA4oA7KpMS6RgOo/1r6f/7aQwPohxIAAByf2g4upCvdUPTpNIA4oA7KUHR1vxpAr/nzPm/6ufrfigIAcHxqO7iQrnRD0afTAOKAOihD0dVpAAEAsBG1HVxIV7qh6NNpAHE0nZJJidXtrgH0w88N+K0oAAAHp7CDC+k+NxR9Og0gjqZTMhTdggYQAAAbUdjBhXSfG4o+nQYQR9MpGYpu4VcD6I8//lz6+5W/np978FtRAAAOTmEH19KVbij6XDtoAP1a3X+Kwh0dlKHoFn41gG689I8ENYehKAAAB6ewg2vpSjcUfS4NII6mgzIU3YK/AgYAwEYUdnAtXekmJZ7o1Q2gX+ualYA7OihD0S1oAAEAsBGFHVxOt7qh6Im0sKEo3NdZGYpuQQMIAICNKOzgcrrVTUqcRasaisJ9nZVJidVpAAEAsBGFHVxRF7uh6Cm0pEkJeFfHZSi6ut01gH6u/reiAAAcn9oOrqi73aTEwbWYSQn4SCdmKLo6DSAAADaitoMr6m63VO6wWsZSOfhIJ2ZSYl37agD9XPdCCQAAjk9tBxfV9W5S4rBaxqQEPKZzMxS9klY+FAUA4BSUd3BdXfImJQ6oBUxKwMM6OpMS19CaJyUAADgF5R1cWve8SYlDaeqTEvBJHaBJibNrtZMSAACchQoPLq2r3lK5g2jSS+XgkzpAkxJn12onJQAAOAsVHlxdt70bpXesid4oDV/SMZqUOK/WOSkBAMCJKPKAQ3ZSmuKN0vANHaZJiTNqhZMSAACcizoP+Fc3v7c0Yjea1lsaAd/TebpR+ixa1Y3SAACcizoPSJe/tzRiB5rQWxoBz9CpulH6+FrPjdIAAJyOUg9Y6Bb4lka8SJN4SyPgqTpeN0ofWSu5URoAgDNS7QH/q7vgfY3bRI+8r3Gwgg7ZWxpxNM3+LY0AAOCkFHzA27oU3te41fSYdzUU1tRpe0sjjqAZv6URAACcmrIPuKvb4QP6wLf1dQ/oA7CJjt0dDdqrZnlHgwAAODuVH/CBrokP62MP62MP62Owrc7ffY3bk2Z2X+MAALgAxR/wkO6LL9VU4HU6i+9q6Os0j3c1FACAy1ACAp/T9XFDPRh2o6P5kUZvpad+pNEAAFyMQhD4iq6SK+thsEsd08f0mWfr2x/TZwAAuCTlIPBdXS6fpC+Fg+jgfl6ff1gf+5K+AgCAC1MUAs/XpfMBfQAOrgO9P80PAIDLUxoCK+oOOhSF8+qsv1qzAQCAQY0IrKjL6FAULqBDv62eDQAANxSLwIq6lQ5F4Xr6DTxb3w4AAB9ROwIr6pI6FAW+1xLqKwAA4GGKSGBF3VaHosAn9RMaigIAwMMUkcCKuq0ORYFP6ic0FAUAgIcpIoEVdVsdigKf16/op0IAAPAwRSSwom6rQ1Hg8/oVDUUBAOAxKkhgRV1Vh6LA5/UrGooCAMBjVJDAirqqDkWBz+tXNBQFAIDHqCCBFXVVHYoCn9evaCgKAACPUUEC6+q2OhQFPqmf0FAUAAAeo4IE1tVtdSgKfFI/oaEoAAA8RgUJrKvb6lAU+KR+QkNRAAB4jAoSWFe31aEo8En9hIaiAADwGBUksK5uq0NR4JP6CQ1FAQDgMSpIYF3dVoeiwCf1ExqKAgDAY1SQwLq6rQ5FgU/qJzQUBQCAx6gggXV1Wx2KAp/UT2goCgAAj1FBAuvqtjoUBT6vX9FQFAAAHqB8BNbVVXUoCnxev6KhKAAAPED5CKyrq+pQFPi8fkVDUQAAeIDyEVhXV9WhKPB5/YqGogAA8ADlI7CurqpDUeDz+hUNRQEA4AHKR2BdXVWHosDn9SsaigIAwAOUj8C6uqoORYHP61c0FAUAgAcoH4F1dVUdigKf169oKAoAAA9QPgLr6qo6FAU+r1/RUBQAAB6gfATW1VV1KAp8Xr+ioSgAADxA+Qisq6vqUBT4vH5FQ1EAAHiA8hFYV1fVoSjwef2KhqIAAPAA5SOwrq6qQ1HgS/oh/VQIAAAeoHwE1tVVdSgKfEk/pKEoAAB8RO0IrKt76lAU+JJ+SENRAAD4iNoRWFf31KEo8CX9kIaiAADwEbUjsK7uqUNR4Ev6IQ1FAQDgI2pHYF3dU4eiwJf0QxqKAgDAR9SOwLq6pw5FgS/phzQUBQCAj6gdgdV1VR2KAp/Xr2goCgAAH1E7AqvrqjoUBT6vX9FQFAAAPqJ2BFbXVXUoCnxev6KhKAAAfETtCKyuq+pQFPi8fkVDUQAA+IjaEVhdV9WhKPB5/YqGogAA8BG1I7C6rqpDUeDz+hUNRQEA4CNqR2B1XVWHosCX9EMaigIAwLsUjsDquqcORYEv6Yc0FAUAgHcpHIHVdU8digJf0g9pKAoAAO9SOAKr6546FAW+pB/SUBQAAN6lcARW1z11KAp8ST+koSgAALxL4QisrnvqUBT4kn5IQ1EAAHiXwhFYXffUoSjwJf2QhqIAAPAuhSOwuu6pQ1HgS/ohDUUBAOBdCkdgdd1Th6LAl/RDGooCAMC7FI7A6rqnDkWBL+mHNBQFAIB3KRyB1XVPHYoCX9IPaSgKAADvUjgCq+ueOhQFvqQf0lAUAADepXAEVtc9dSgKfEk/pKEoAAC8S+EIrK576lAU+Kp+S0NRAAC4T9UIrK5L6lAU+Kp+S0NRAAC4T9UIrK5L6lAU+Kp+S0NRAAC4T9UIrK5L6lAU+Kp+S0NRAAC4T9UIrK5L6lAU+Kp+S0NRAAC4T9UIrK5L6lAU+Kp+S0NRAAC4T9UIrK5L6lAU+Kp+S0NRAAC4T9UIrK5L6lAU+Kp+S0NRAAC4T9UIbKF76lAU+JJ+SENRAAC4T9UIbKF76lAU+JJ+SENRAAC4T9UIbKF76lAU+JJ+SENRAAC4T9UIbKF76lAU+JJ+SENRAAC4T9UIbKF76lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+Kp+S0NRAAC4Q8kIbKFL6lAU+IZ+TkNRAAB4i3oR2EI31KEo8LB+PA/rYwAA8JMCEdhCV9KhKHBHP5Wn6qsBALgk5SCwhS6gQ1Fg6LexoR4MAMA1qP+ALXTjHIrC5fWTeLVmAwDAean5gC10yxyKwlX1S9iZJgcAwBmp9oAtdL8cisLF9AP4kr5iUmIoOhT9kr4CAIATUeQBW+haORSFy+jof0afvK9xQ9H7GvewPgYAwCko74AtdKEcisIFdOgf0Ac+o09+/rN97AF9AACAg1PYAVvoKjkUhVPruH+k0a/TPN7VUAAADktJB2yke+RQFM6oU/6uhu5G03pXQwEAOCDFHLCRbpBDUTidjvgdDdqxJnpHgwAAOBqVHLCRro9DUTiRDvcdDTqIJn1HgwAAOA41HLCRLo5DUTiLTvZbGnFALeAtjQAA4CAUcMBGujUOReH4OtNvacTBtZi3NAIAgN1TugEb6b44FIWD60C/pRGn0JLe0ggAAPZN3QZspMviUBSOrNN8o/TptLwbpQEA2DFFG7CRbopDUTisjvKN0ifVIm+UBgBgr1RswEa6Jg5F4Zg6x0vlLqAFL5UDAGCXlGvA83Ud/Eij4Wg6wUvlLqNlL5UDAGB/1GrAt3Tte56+F/aqk7pU7mJa/FI5AAB2RqEGfE6XvK30VNiNjuakxCW1BZMSAADsjEINeEh3u9dpHvBSHcdJiQtrIyYlAADYE1Ua8J7uc3vSzGBzHcFJictrOyYlAADYDSUa8LaucZ/X52+UHt4Mfsqvb4DNdPImJS6v7ZiUAABgN5RowP/qAveYPvOYPvPupxrxmD4D6+vMTUrwU5syKQEAwD6oz4Dfurd9pNHr63kfaTSspqM2KcGkrZmUAABgBxRnwL+6rr2roa/QDN7VUFhBh2xSgklbMykBAMAOKM7g6rqo3de4fWhO9zUOnqezNSnBjTZoUgIAgFdTmcGldUW7o0H70/zuaBA8SQdrUoIbbdCkBAAAr6Yyg+vqfvaWRuxbc31LI+AZOlVDUe5om4aiAAC8msoMrqib2VsacRzN+y2NgG/oME1KcEfbNCkBAMBLKcvgcrqT3Sh9TK3hRmn4qk7SUHT//vn7rz//+KNZ/9///fHnX3//U2p9PXQoCgDASynL4Fq6kN0ofWSt5EZp+JKO0VB05/7+a7R+/vih//l///fn3+XX1vOGogAAvJSyDC6k29iN0sfXem6Uhs/rDA1Fd+7vP5d/4uef0RDaqAX062H/KQoAwEspy+AquootlTuX1rZUDj6j0zMpcTi1gP74a6u/B/Zzt34rCgDA66jJ4BK6hC2VO6NWuFQOHtbRGYq+XN2c5R/n+fvP29hvGkAAAJenJoNL6BI2KXFerXOpHDymczMU3YFf3Z6p3fN+++eND6zt5+N+KwoAwOuoyeD8uoFNSpxdq52UgMd0boaiu7Do6HzU/mnAdn/+RwMIAGB/1GRwcl2/JiWuoTVPSsADOjRD0Z0YTZ2/H2z/bPjHf374+cTfigIA8DpqMji5rl+TEpfRsicl4COdmKHobtTY+df95s4/r2j/aAABAOyPmgzOrLvXpMSVtPJJCfhIJ2Youh9///nR/7173Z8t//LXLz8f+1tRAABeR00Gp9XFa1Lielr/pAS8q+MyFN2L/p+9fnmrBVR/aPvuzw8/J/VbUQAAXkdNBqfVxWtSYnv//P3Xn3/8vqv+8edff296I+25kxLwro7LUHQffrV//m3u/PpTPv/b5nll9+eHf589KQoAwOuoyeC0ungNRbf3939/TOGPH/qf/oskHEHHZVLi5frTP/2MblpAL+7+/PDv4ydFAQB4HTUZnFO3rkmJ7f24ii7+xM9/f29l0xbQr0fOSsC7Oi5D0Rdbtn/+tWgB9R/++dVz/R+b/Ox6/FAUAICXUpbBOXXxGoruRLfXrf90ws+d+K0ovKvjMhR9qf5/vZatnP96Qj9+Vf81gN6wyc+uZw1FAQB4KWUZnFMXr6HoGm7/LMIPb11Qf9MA4jg6LkNR3tVmDUUBAHgpZRmcUxevoeg6+uMGv9s977d/3vjANn4+dKEE3NdZmZTgjrZpUgIAgJdSlsEJdesaiq5o0dH5qP3TgJf852n/ffCkKLyr4zIU5Y62aSgKAMCrqczghLp4DUVXNZo6fz/Y/tn8j//88vPRvxWFd3VcJiV4S3s0FAUA4NVUZnBCXbyGoiursfOv+82d/uu1L2r/uJryJR2XSQlutEGTEgAAvJrKDE6oi9dQdG1///nzP+38Tnen7s9L/vLXLz+f/1tR+EgnZlKCpXZnKAoAwA4ozuCEunsNRdfV/7PXL2+1gOoPvbD788PP2f1WFD7SiZmUYNLWTEoAALADijM4oe5eQ9E1/Wr//Nvc+fWnfP63zbOL7s8P/05iUhQe0KGZlOCnNmVSAgCAfVCfwQl1/RqKrqc//dOf+7lpAe2l+/PDv/OYFIUHdGiWyqEBBACwe+ozOKGuX0PRtSzbP/9atID6D//8+Pdbm/+3oJvKUBQe07mZlLi8tmNSAgCA3VCiwQl1A5uUWEH/v17LVs5/PaF/pgbQG7b9Q0E9dFICHtbRmZS4sDZiUgIAgD1RpcE5dQ8bil5bezEUhU/qAE1KXFJbsFQOAIA9UaXBOXUPG4peW3sxFIVP6gAtlbuYFr9UDgCAnVGowTl1FZuUuKp2YVICPq8ztFTuMlr2UjkAAPZHrQan1YVsKHpV7cJQFL6qk7RU7gJa8FI5AAB2SbkGp9WdbFLiktqCoSh8Q4fpRumTapE3SgMAsFcqNjitrmWTEtfT+icl4Hs6TzdKn07Lu1EaAIAdU7TBmXU5m5S4klY+KQHP0Kl6SyPOolXdKA0AwL6p2+DMup9NSlxJK5+UgCfpYL2lEQfXYt7SCAAAdk/pBifXLW1S4hpa86QEPFsn7C2NOKAW8JZGAABwEAo4OL+ua5MSZ9dqJyVgHZ2zOxp0EE36jgYBAHAcajg4v25sS+XOq3UulYM1ddruaNCONdE7GgQAwNGo5OASurotlTujVrhUDtbXmXtXQ3ejab2roQAAHJBiDq6iC9xSuXNpbUvlYEMdvnc19KWayrsaCgDAYSnp4EK6yd0ofXyt50ZpeIVO4QP6wCZ65AP6AAAAB6ewg2vpSnej9JG1khul4aU6jg/rY0/VV39GnwQA4PjUdnA5XexulD6m1nCjNOxD5/Kr+pbH9Jnv6bsAADg+tR1cUXe7tzTiOJr3WxoB+9MZPYJmDADAwSns4Lq63r2lEfvWXO9oEOxYh3UHmtBbUyoBAMDBKezg0rrh3dGg/Wl+dzQIDqXju6EefKP0pAQAAEemqoOr64Z3X+P2oTnd1zg4uA70s/Xt72roUjkAAA5LSQf8q0veuxr6Cs3gXQ2FE+lwf1Xf8nl9fqkcAADHpJ4Dfuue95FGb6JHvquhcC6d70mJTfTIpXIAAByQYg5Y6J73sD72PH3vJ/VhOJEO91B0Qz14UgIAgANSzAFv6Lb3eX3+M/rk9/RdcCId7qHotnr2pAQAAEejkgPe051vT+5N7FcczqFjPSmxuR4/KQEAwKEo44CPde17qaYyKTEpAcfXmR6KvkIzWCoHAMBxqOGAD3The5Em8ZZGTErAwXWgJyVepEkslQMA4CAUcMAHuu0NRX8q9Gx9+wP6wKQEHFmneSj6Uk1lUgIAgINQwAHv6ao3KXFf4x7TZ76qb5mUgCPrNA9FX63ZTEoAAHAEqjfgPd3zhqJ70swmJeCYOseTEjvQhCYlAADYPaUbcFc3vEmJnWlykxJwQB3ioeg+NKelcgAA7Ju6Dbir692kxM40uUkJOJpO8KTEbjStpXIAAOyYog24q7vdUHSXmuKkBBxKx3coujNNblICAIAdU7QBb+tiNymxV81yUgKOo7M7FN2f5jcpAQDAXqnYgLd1qxuK7lgTnZSAg+jgTkrsUlOclAAAYJeUa8Abus9NSuxbc52UgCPo1A5F96pZLpUDAGB/1GrAG7rMDUWPoBlPSsDudWSHojvWRJfKAQCwMwo14H91jZuUOIJmPCkB+9Z5nZTYt+Y6KQEAwM4o1ID/1TVuKHoczXtSAnaswzoUPYJmPCkBAMCeqNKAhS5wkxKH0tQnJWCXOqaTEgfRpCclAADYDSUasNDtbSh6NM1+UgJ2qWM6FD2O5r1UDgCAfVCfAb91b5uUOKAWMCkB+9MZHYoeSlNfKgcAwA4ozoDfurQNRQ+rZUxKwJ50OicljqbZL5UDAODVVGbAb93YhqKH1TImJWBPOp1D0WNqDZMSAAC8msoMSNe1SYkjayWTErAbHc2h6GG1jEkJAABeSlkGpLvaUPT4Ws+kBOxAh3JS4rBaxlI5AABeR00G/Ktb2qTE8bWeSQnYgQ7lUPTgWsxSOQAAXkRBBvyrK9pQ9Cxa1aQEvFTHcVLi+FrPUjkAAF5BNQZcoj/SwiYl4HU6i0PRs2hVkxIAALyCagw4+S30l9Y2KQGv01kcip5IC5uUAABgc0oxuLquZZMSp9PyJiXgFTqFkxIn0sKWygEAsC11GFxdd7Kh6Em1yEkJ2FxHcCh6Oi1vqRwAABtShMGldRublDipFjkpAdvq/E1KnFErXCoHAMBWVGBwaV3FhqKn1lInJWBDHb6h6Hm1zkkJAAC2ogKD6+oeNilxdq12UgK20skbip5aS52UAABgE8ovuK4uYZMSZ9dqJyVgEx27SYlTa6lL5QAAWJ/aC66rG9hQ9Bpa86QErK8zNxS9gBa8VA4AgJUpvOCiuntNSlxGy56UgJV14Iai19Cal8oBALAmVRdcVBevoeiVtPJJCVhTp21S4jJa9qQEAABrUnXBFXXrmpS4mBY/KQGr6agNRS+mxU9KAACwGiUXXFFXrqHoJbUFkxKwgg7ZpMT1tP5JCQAA1qHegsvpsjUpcUltwaQErKBDNhS9pLZgqRwAACtQbMHldNMail5YGzEpAc/WCRuKXlW7sFQOAIBnU2nBtXTHmpS4tvZiUgKep7M1KXFhbcSkBAAAz6bSgmvpjjUUvby2Y1ICnqezNRS9vLZjUgIAgKdSZsGFdLualMDmsLJO1aQENgcAYBNqLLiQrlZDUYb2ZVICvq0jNRTlpzZlqRwAAE+iwIIL6V41FGVoXyYl4Ns6UkNRhvZlqRwAAM+guoKr6EY1KcGkrZmUgG/oME1KMGlrJiUAAHgG1RVcRTeqoSg32qBJCfiqTtJQlBtt0KQEAADfprSCS+guNSnBjTZoUgK+qpM0FOUt7dGkBAAA36OugkvoIjUU5Y62aVICPq8zNCnBW9qjpXIAAHyDogrOryvUpAR3tE2TEvB5naGhKPe1U0vlAAD4KhUVnF/3p6Eo72qzJiXgMzo9kxK8q81aKgcAwJcop+DkujlNSvCR9mtSAh7W0RmK8oC2bFICAIAvUU7ByXVzGorygLZsUgIe1tEZivKYdm1SAgCAz1NLwZl1Z5qU4DHt2qQEPKBDMynBY9q1pXIAAHySQgrOrAvTUJTPaO8mJeAjnZihKJ/R3i2VAwDgM1RRcFpdlSYl+Iz2blIC3tVxmZTgk9q+pXIAADxMCQWn1T1pUoJPavsmJeC+zspQlC9pEyclAAB4mBIKTqt70lCUL2kTJyXgjg7KUJSvah8nJQAAeIz6Cc6pG9KkBF/SJk5KwFs6JZMSfFX7uFQOAIAHKJ7gnLoeDUX5hrZyUgJudESGonxPu7lUDgCAj6ic4IS6GE1K8D3t5qQELHU+hqJ8Wxu6VA4AgHcpm+CEuhUNRfm2NnRSAiYdjkkJnqE9nZQAAOBdyiY4m65EkxI8Q3s6KQFDJ2MoyvO0s5MSAADcp2aCs+k+NBTledrZSQlwPDbRzi6VAwDgDgUTnEo3oUkJnqednZSAm+NRlGdrf5fKAQDwFtUSnErXoKEoz9b+TkpweR2IoSgraIuXygEAcEOpBOfRBWhSghW0xZMSXFhHYVKCdbTLkxIAANxQKsF5dAEairKOdnlSggvrKAxFWVN7PSkBAMCSOglOoqvPpASraaMnJbiqzsFQlJW13ZMSAABMFElwEt17JiVYU3s9KcH1dAImJVhZ271UDgCAQYUEJ9GlZyjKytruSQmupxMwFGUTbfpSOQAAflIewRl03ZmUYH3t+KQEV9K7n5RgK+37pAQAAD8pj+AMuu4MRdlK+z4pwWX04oeibKvdn5QAAEADCE6gi86kBFtp3ycluIxe/FCUzfUCJiUAAC5PYQSH1y1nKMq22v1JCS6gVz4pweZ6AUvlAACuTVUEx9b9ZlKCzfUCJiU4u973UJQX6TUslQMAuDAlERxbl5uhKK/QO5iU4NR62ZMSvE5vYlICAODClERwYN1sJiV4kV7DpATn1ZseivJqvY9JCQCAq1IPwYF1rRmK8lK9jEkJTqrXPBRlB3olkxIAAJekGIKj6kIzKcFL9TImJTij3vGkBDvQK1kqBwBwPSohOKpuM0NRdqBXMinB6fSCh6LsRi9mqRwAwMUog+CousoMRdmH3sqkBOfS2x2Ksie9m6VyAABXogaCQ+oSMynBPvRWJiU4kV7tpAQ70+uZlAAAuBI1EBxSl5ihKHvSu5mU4Cx6r0NRdqmXNCkBAHAZCiA4nq4vkxLsTK9nUoLj641OSrBLvaSlch9p9LsaCgCwY0oWOJ4uHENR9qc3NCnB8fVGh6LsWK9qqdykxLf1dQAAu6FAgYPpbjEpwS71kiYlOLhe51CUfettLd2LP9GvpwMAvJaiBA6m+8RQlB3rVU1KcFi9yEkJdq8X9iJNAgDgFdQicCTdISYl2LFe1aQEB9QrvKNB7Ftv63WaBwDAtlQhcCTdHoai7F4v7CONZjd6Md/QF7EbvZjP6JPvauhn9EkAgK2oP+AwujRMSrA/vaFn6BvZSvu+gh7Ai/QaHtAHvqEvekAfAABYn8oDDqPrwlCU3ejFrKknsYK2eBM9kq207+9q6Ap6wLsaCgCwJjUHHEO3hEkJXq33sa2ezTO0p5vr8ays7X5XQ1fWw+5oEADAahQccAxdESYleJ3exOs0D76qfXxYH3tXQx/Wx1hBW3xHgzbX4+9oEADACpQacAxdDoaivELv4JP68Eca/Ul9mIe1cR9p9Df0RR9pNM/Tzr6lES/VVN7SCACAZ1NnwAF0LZiUYHO9gAf0ge/pux7TZ3hXm/Wuhj5VX/2uhvJtbeiN0rvRtG6UBgB4KkUGHEB3gqEo22r339XQdfSMdzWUO9qmOxq0sh52R4P4hrbyRumdaXI3SgMAPI8KA/au28CkBFtp3+9r3FZ66n2NY6ndeUsjNtSD39IIvqRNXCq3Y010qRwAwJMoL2DvugoMRdlK+35Hg16hGdzRIH5qU97SiBdpEm9pBJ/R3i2V272mu1QOAOAZ1Bawa10CJiVYXzt+R4Nerdnc0aBray9ulN6BJnSjNI9p15bKHUSTXioHAPBtCgvYqWr/OxrEatrotzRiT5rZWxpxVe3CjdK70bRulOYj7ddSuUNp6kvlAAC+R1UBO1Kx/3l9nidpW2+U3qtmeaP09bT+G6X3p/ktleO+dmqp3AG1gKVyAADfoKSAF6u6f5K+lG9oK2+U3rfmeqP0lbTypXI71kSXynFH2zQpcVgtY1ICAOAblBTwMtX1K+gBfF47eKP0ETTjG6WvoTUvldu9prtUjhtt0KTEwbWYSQkAgK9ST8BrVNGvpsfwGe3dUrmjafZL5c6u1S6VO4gmvVSOSVszKXEKLWlSAgDgSxQTsLUK+Y80+o4GfaTRPKZdm5Q4ptYwKXF2rXZS4lCa+qQEk7ZmUuIUWtKkBADAlygmYDuV8O9q6Gf0yfsax7varEmJI2slkxLn1TonJQ6oBUxK8FObMilxIi1sUgIA4PNUErCRive3NOLb+rq3NII72qZJieNrPZMSZ9QKJyUOq2VMSnCzOUVPp+UNRQEAPk8lAVuocn9LI56kL31LI3hLezQpcXytZ6nc6bS8SYnDahmTEpfXdkxKnE7Lm5QAAPgkZQSsrpr9RukV9IAbpVlqdyYlzqJVTUqcS2ublDi4FjMpcW3txVD0pFrkUBQA4JOUEbCuCvalcivrYUvlmLQ1Q9FzaW2TEifSwoaip9CSJiUurI0Yip5Ui5yUAAD4DDUErKhSfancJnrkUjl+alMmJU6n5Q1Fz6JVTUqcQkualLiqdmEoemotdSgKAPAZaghYS3X6UrkN9eClcrzyWvX3nz3zf/z5dwOerK8fip5FqxqKPttff/T99/zx1z8NfbYeMBS9qnZhKLq1TX/CffdQFADgM9QQsIqK9KVym+vxS+Wurb2YlNjCuD3+8T/+WqkB9MOvB/6n6PG1nkmJZ/v7z17STz2rf/vpz783agD9UOKS2oKh6NY2/Qn/etR/igIAfIYaAp6vCn2p3Is0iaVyF9ZGDEU38uv2uNaf93nTz1X+VvT4Ws9QdG3//PzzQOv9mZ8bv1b3n6LX0/onJba29U/451p/KwoA8DAFBDxf5fmkxEs1lUmJC2sjhqIb0QB6mtYzFF3bqxtAP5S4mBY/FH0BDSAA4GAUEPBk1eaTEjvQhCYlLqktmJTYyOsbQD+UOLJWMhTdwOYNoB9+rfE/RS+mxQ9FX0ADCAA4GAUEPFm1+VB0N5rWUPSS2oKh6HZe0AD64edafyt6ZK1kKLoBDaAXafFD0RfQAAIADkYBAc9UYT4psRtNa1Lielr/UHQ7v26Pf/zx59Jq/xXhX36u9beiR9ZKhqIb0AB6kRY/FH2BrX/CP5f7W1EAgIcpIOCZKsyHojvT5Iai19P6h6Lb+XV7vLHynyfoKUPRI2slQ9EN7KAB9EOJK2nlQ9EX2Pon3PcPRQEAHqaAgKepKp+U2JkmNylxMS1+KLqdrf/+yC8/1/pb0cNqGUPRbbyiAfTDr5X+p+iVtPKh6Ats/RP+udzfigIAPEwBAU9TVT4U3aWmOBS9mBY/FN2OBtATtIyh6DY0gF6klQ9FX0ADCAA4GAUEPEcl+aTELjXFSYkraeVD0e1oAD1ByxiKbkMD6EVa+VD0BTSAAICDUUDAc1SSD0V3rIkORa+klQ9Ft6MB9AQtYyi6DQ2gF2nlQ9EX0AACAA5GAQHPUUk+FN2xJjoUvZJWPhTdjgbQE7SMoeg2NIBepJVPSmxNAwgAOBgFBDxB9fikxI410UmJy2jZkxIbeUED6OcqF0ocVssYim5DA+h1WvxQdGub/oR/LvS3ogAAn6GGgCeoJB+K7l7THYpeSSsfim7k9Q2gokfWSoai29AAep0WPxTdmgYQAHAwagh4gkryoejuNd2h6JW08qHoebXOoeiRtZKh6Km11KHoxbT4oeiptdShKADAZ6gh4AkqyYeiu9d0h6JX0sonJU6qRQ5Fj6yVDEVPraUORS+mxU9KnFSLnJQAAPgMNQQ8QSX5UPR7fv71kvc85a+e9F1D0Stp5UPRM2qFQ9GDazFD0VNrqUPR62n9Q9GTapFDUQCAT1JGwHdVkg9Fv+3vP/+Y9OX9209//q0B9AStfFLidFreUPT4Ws9Q9KRa5KTE9bT+SYnTaXmTEgAAn6SMgO+qJB+KPtdq/7nZX3P+T9EraeWTEufS2iYljq/1DEVPqkUORa+qXRiKnk7LG4oCAHyeSgK+q6p8KPpcGkBravGTEmfRqiYlTqElDUVPqkUORa+qXZiUOJEWNikBAPB5Kgn4rqryoehzaQCtqcUvlTu+1rNU7hRa0lD0pFrkUPTC2ohJiVNoSUvlAAA+TyUB31VVPhR9Lg2glbX+SYnjaz2TEifSwoaip9PyJiUurI2YlDiFljQpAQDwJYoJ+K4K86Hoc2kAra8tmJQ4slYyKXEurW0oejotbyh6eW3HpMTBtZhJCQCAr1JPwHdVmw9Fn0sDaBPtwqTEMbWGSYnTaXmTEifSwiYlOOPmtIxJCQCAb1BSwHdVng9Fn0sDaBPtwlK5o2n2S+XOqBUORU+khQ1F+alNWSp3QC1gqRwAwDcoKeC7Ks+Hos+lAbSVNmKp3HE076VyJ9UiJyVOoSVNSjC0L0vlDqWpL5UDAPgeVQU8QUX6UPSJNIA21F7cKL1vzfVG6fNqnZMSp9CSJiWYtDVL5Q6iSS+VAwD4NoUFPEF1+lD0idZpAP2a7X+Kcuca9kPpvWqWN0qfXaudlDi4FjMpwY02aKnc7jXdpXIAAM+gtoAnqFQfij6RBtDm2pS3NGJPmtlbGnENrXlS4rBaxqQEd7RNS+V2rIkulQMAeBLlBTxB1fpQ9Ik0gF6hfXlLI/ahOb2lEZfRsiclDqtlTEpwXzt1o/TONLkbpQEAnkeFAU9QwT4p8SwrNIB+zXNWgqV25y2NeJ3mcUeDLqbFT0ocUAuYlOAj7deN0rvRtG6UBgB4KkUGPEdl+1B0x5roUJS3tEf3NW4rPfW+xl1VuzApcShNfVKCx7Rrb2nESzWVtzQCAODZ1BnwHFXuQ9Eda6JDUe5rp97V0NX0mPsad23txVK5g2jSS+V4WBt3R4M21+PvaBAAwAqUGvAcFe+TErvUFCcleFeb9YA+8Ax94wP6AHc2rdzuNd2lcnxeO3hHgzbRI+9oEADAahQc8DRV8UPRXWqKQ1Ee0659Rp98QB/4pD7MpK1ZKrdjTXSpHF/VPt7XuGfoG5ffWei+xgEArEnNAU9TIT8psTNNblKCz2jvXq3Z8Jb2aKncLjXFpXJ8Wxv6kUZ/SV/xGX0SAGB9Kg94pir6oejONLlJCb6kTdxWz+Yj7deN0rvRtG6U5nna2Yf1sQf0gc/okwAAm1B8wDNV1E9K7EbTmpTg29rQlfUwHtbG3Si9A03oRmlW0Ba/SJMAANiWKgSerAJ/KLobTWsoyrO1v8/QN/INbeVbGvEiTeItjWBN7fWGejAAwCuoReDJKvMnJXagCU1KsL52/DF9hudpZ+9o0IZ68B0NYivt+2p6DADASylK4Pkq+SclXqqpTEqwA72SoSjP1v7e0aCV9bA7GsTr9CaepC8FANgBpQk8X4X/UrkXaRJL5diBXsmkBM/W/t7XuBX0gPsax870et7V0JvBRQEAdkBpAquo9l8qt7kev1SO3ejFDEVZR7v8kUb/VOiTr6bPfKTRHF9vdCgKALADShNYS+X/UrkN9eClcuxJ72Yoypra68/r83c06DF9hrPovQ5FAQB2QGkCK+oGsFRuEz1yqRw70+sZirK+dnxzPZ5z6e0ORQEAdkBpAuvqErBUbmU97EZpdqbXMxRlK+37JnokZ9Q7HooCAOyA0gRW1z3gRukV9IAbpdmlXtJQlM31AlbQAzi1XvZQFABgB5QmsIWuAm9pxJP0pW9pBHvVexqK8jq9iW/r67iG3vpQFABgB5QmsJFuA29pxLf1dW9pBPvW2xqKsgO9kgf0Aa6qczAUBQDYAaUJbKcLwX2Nm5R49xbRiPsax+71woai7ECvZCgKb+mUDEUBAF5NXQJb607wkduRvz7+n6IfaTRH0DsbirIDvZKfCsEdHZShKADAq6lL4AW6Fqysh3EcvbmhKK/W+xiKwh0dlKEoAMCrqUvgZbocrKAHcDS9v6Eor9b7GIrCHR2UoSgAwKupS+DFuiI8SV/KMfUWh6K8Wu9jKAp3dFCGogAAr6Yugb3orvAlfQUH1+uclOClehlDUbijgzIUBQB4NXUJ7FRXhzsaxOn0goeivFQvYygKd3RQhqIAAK+mLoFd6wIxFOW8etNDUV6qlzEUhTs6KENRAIBXU5fArnWBGIpyXr3poSgv1csYisJ9nZWhKADASylKYNe6PQxFOa/e9FCUl+plDEXhvs7KUBQA4KUUJbBr3R6GopxXb3ooykv1MoaicF9nZSgKAPBSihLYtW4PQ1HOqzc9FOWlehlDUbivszIUBQB4KUUJ7F0XiKEo59WbHoryOr2JoSjc11kZigIAvJSiBPauC8RQlPPqTQ9FeZFew1AU3tVxGYoCALyUogT2rgvEUJTz6k0PRXmRXsNQFN7VcRmKAgC8lKIE9q4LxFCU8+pND0V5kV7DUBTe1XEZigIAvJSiBPauC8RQlPPqTQ9FeZFew1AUPtKJGYoCALyOigT2rtvDUJTz6k0PRXmRXsNQFD7SiRmKAgC8jooEDqALxFCUk+o1T0rwCr2DoSh8pBMzFAUAeB0VCRxAF4ihKOfVmx6K8gq9g6EofKQTMxQFAHgdFQkcQBeIoSjn1ZseivIKvYOhKHykEzMUBQB4HRUJHEAXiKEo59WbHoryCr2DoSh8pBMzFAUAeB0VCRxAF4ihKOfVmx6K8gq9g6EofKQTMxQFAHgdFQkcQBeIoSjn1ZseivIKvYOhKHykEzMUBQB4HRUJHEAXiEkJTqrXPBTlFXoHPxWCB3RohqIAAK+jIoFj6A4xFOW8etNDUbbV7g9F4TGdm6EoAMCLKEfgGLpADEU5r970UJRttftDUXhM52YoCgDwIsoROIYuEENRzqs3PRRlW+3+UBQe07kZigIAvIhyBI6hC8RQlPPqTQ9F2Va7PxSFx3RuhqIAAC+iHIFj6AIxFOW8etNDUbbV7g9F4TGdm6EoAMCLKEfgGLpADEU5r970UJRttftDUXhM52YoCgDwIsoROIzuEENRTqrXPCnBhtr6oSg8pnMzFAUAeBHlCBxGd4ihKOfVmx6KsqG2figKj+ncDEUBAF5EOQKH0R1iKMp59aaHomyorR+KwsM6OkNRAIBXUIvAYXSBGIpyXr3poSgbauuHovCwjs5QFADgFdQicBhdIIainFdveijKhtr6oSg8rKMzFAUAeAW1CBxGF4ihKOfVmx6KspX2fSgKn9HpGYoCALyCWgSOpDvEUJST6jUPRdlK+z4Uhc/o9AxFAQBeQS0CR9IdYijKefWmh6Jsok0fisJndHqGogAAr6AWgSPpDjEU5bx600NRNtGmD0XhMzo9Q1EAgFdQi8CRdIcYinJevemhKJto04ei8BmdnqEoAMArqEXgSLpDDEU5r970UJRNtOlDUfikDtBQFABgcwoROJIuEENRzqs3PRRlE236UBQ+qQM0FAUA2JxCBI6kC8SkBCfVax6Ksok2fSgKn9QBekyfAQBYgVIDDqZbwlCUk+o1T0qwvnZ8KAqf0en5kr4CAOBJlBdwMN0MhqKcV296KMr62vGhKHykE/NUfTUAwDcoKeBgug0MRTmv3vRQlPW140NRuK+zspoeAwDwJYoJOJjuAUNRzqs3PRRlZW33UBTu6KBsokcCAHySMgIOphvAUJTz6k0PRVlZ2z0UhRsdkQf0gXc19AF9AADgYQoIOJhq/6Eo59WbHoryDW3lZ/RJWOp83Ne4L+kr7mscAMBjVA9wPNX+Q1FOpxf8robykfbrq/oWGDoZdzToSfrSOxoEAPARdQMcT1X/UJRT6KV+SV/B0L48VV/NtXUa3tKIFfSAtzQCAOBdigY4nkr+oSiH1Yt8qr76qtqF1fQYLqlDcKP0ynrYjdIAAPepGOB4qveHohxQr3A1PeZKWvkmeiRX0ru/UXoTPfJGaQCAO5QLcDwV+0NRDqWXt5Weemot9QF94F0NfUAf4AJ65UvlNtfjl8oBALxFrQDHU6X/kUazM72eB/SBm48U/anQA/rASbXI+xr3JX3FfY3j1HrZS+VepEkslQMAuKFQgMOouv+qvoXX6U28q6E3Sr/7HhvxroaeSAu7o0FP0pfe0SBOqtc8KfFSTWVSAgDghkIBdq2K/tn6djbU1t/RoCfpS+9o0Cm0pLc0YgU94C2N4HR6wZMSO9CEJiUAAJZUCbBHVfHr63msrO1+SyNW0APe0oiDazE3Sq+sh90ozYn0aicldqNpTUoAAEyUCLAvFe/b6tmso12+UXplPexG6cNqGTdKb6JH3ijNWfReJyV2o2lNSgAATJQIsBeV7V/SV9wo/Zg+w1O1uUvlNtSDl8odUAtYKre5Hr9UjuPrjU5K7EyTm5QAABjUB7ALFezvauhQ9OEqv9EfaTTP0J4uldtcj18qdyhNfancizSJpXIcXK9zKLpLTXEoCgAwqA/gxSrV72vc8/S99zWO72k3l8q9SJNYKncQTXqp3Es1laVyHFYvclJil5ripAQAwE+KA3iZKvQ7GrSmnnRHg/iSNnGp3Es1laVyR9CMJyV2oAlNSnBYvcih6I410aEoAMBPigN4jcrztzRiKz31LY3g89rBSYkdaEJL5fatuU5K7EbTmpTgmHqLQ9Eda6JDUQCAnxQH8ALV5jdKv0IzuFGaz2jvJiV2o2lNSuxYE52UWMHff/35519/9y+f1OQmJTigXuFQdMea6FAUAOAnxQFsrcL8RunXaR43SvOYdm1SYmea3KTEXjXLSYnn++evP/7v//7465/+9XOa3KQER9P7G4ruXtMdigIA/KgT+iewiUrypXL70JyWyvGAtmxSYn+a31B0r5rlUHQV32oA/dAUh6IcTe9vKLp7TXcoCgDwo07on8D6qseXyu1JM1sqx0far6Hoyr72V5aa4lB0l5ripMQqntwA+qEEh9LLG4ruXtMdigIA/KgT+iewvurxSYn9aX5L5bivnZqUWNcXGxZNcVJif5rfUHQt320A/dBEh6IcSi9vKLp7TXcoCgDwo07on8DKKsYnJfaqWU5KcF87NRRd3dcbFk10KLo/zW8o+kz//iGq//zYzh8b2r/867vNtaIcSi9vKPpdi5P2w1f/a+N3Nd2hKADAjzqhfwJrqhKflNi35jopwR1t01B0dRpA3/ZzC+/79Ob2uUkJjqM3NxT9ppuT9r0/avaGvncoCgDwo07on8CaqsQnJfatuS6V40YbNCmxum/9laXmOhTdk2Y2FF3Rt/bzP013KMpx9OaGorvXdIeiAAA/6oT+CaymMnxS4gia8aQEN9qgoehanvZXln5O9reie9LMhqIr0gDiX725oejuNd2hKADAjzqhfwKrqQwfih5H856UYKndGYqu5Hl/ZalPDEX3pJkNRVekAUR6eUPRHWuiQ1EAgJ8UB7CuyvBJiUNp6kNRltqdoegW/BWw59IAIr28oeiONdGhKADAT4oDWFdl+FD0aJr9UJSldmcougUNoOfSACK9vKHojjXRoSgAwE+KA1hRNfikxNE0+0kJJm3NUHQLGkB71HSHohxKL29SYpea4qQEAMBPigNYUTX4UPSYWsNQlElbMxTdggbQHjXdoShH0/sbiu5SUxyKAgAM6gNYUWX4UPSYWsNQlElbMymxupM3gH5ockPRfWuuQ1GOpvc3KbEzTW5SAgBgUB/AiirDh6LH1BqGoiy1O0PRHWuiQ9H9aX5D0R1rokNRjqm3OCmxG01rUgIAYKJEgLVUhg9Ft/X3X3/++dff/cu3tZKhKJO2Zii6Y010KLo/zW8oumNNdCjKMfUWJyV2o2lNSgAATJQIsJbK8KHopp7zf2b0n1YyFGXS1kxK7FWzHIruT/OblNilpjgpwWH1IicldqAJTUoAACypEmAtVeJD0U1pAL1AuzMU3aWmOCmxS01xKLpLTXEoysH1OiclXqqpLJUDAFhSJcBaqsSHopvSAHqBdmcouktNcSi6V81yUmJnmtykBAfX61wq9yJNYqkcAMANhQKspWJ8KLopDaAXaHcmJXamyU1K7FWznJTYmSY3KcHx9UaXym2uxy+VAwB4i1oB1lI9PhRd3b//3ef//PHvk//oX/71rWbQz3X8VpQbbdCkxG40rUmJfWuukxK70bQmJTiL3uuN0pvokTdKAwDcoVyAtVSSD0XX9vMP/dz3rT8O1HcMRbnRBi2V24EmtFRu95rupMQONKFJCc6lt3uj9Mp62I3SAAD3qRhgLVXlQ9FN+StgL9MeLZV7qaayVO4ImvFSuZdqKkvlOJ1e8FsasYIe8JZGAAC8S9EAK6o2H4pu55kNoNYwFOW+dmqp3Is0iaVyx9G8l8q9SJNYKsdJ9ZrvaNCT9KV3NAgA4CPqBlhR5flQdDsaQC/WZi2V21yPXyp3NM1+qdzmevxSOc6u931f476kr7ivcQAAj1E9wIoq0oei29EAer32a6nchnrwjdIH1AKWym2oBy+V4xp66w/oA+9q6AP6AADAwxQQsKLq9EmJjTytAdTsJyV4QFt2o/TKetiN0kfWSm6UXlkPu1Gai+n1b6JHAgB8kjIC1lXBPhQ9mmY/FOVhbdxbGrGOnnGj9PG1nrc0YgU94C2N4Ko6B6vpMQAAX6KYgHVVtg9Fj6bZD0X5jPbujgY9SV96R4POolXd0aAn6UvvaBCX14F4qr4aAOAblBSwror3SYnjaN6TEnxS2/euhn5JX/Guhp5La3tXQ7+kr3hXQ2Gp8/ElfQUAwJMoL2B11fKTEkfQjCcl+Kr28QF94F0NfUAfOK/W+YA+8K6GPqAPwEc6Me9qKADACpQasIVK+0mJ3Wu6kxJ8Q1u5oR58AS14Kz0VAAB2T/EKW+iyOCmxb811UoInaVtX02Oup/WvpscAAMBxqGJhI10cJyX2qllOSvBs7e9T9dXX1l48VV8NAABHo5aF7XSDnJTYn+a3VI7VtNHf0Bex1O58Q18EAACHpaiF7XSVXCq3J81sqRwbauvf1VA+o717V0MBAOAs1LiwqS6XS+X2oTktlQMAAOCY3Otga/VUbpR+neZxozQAAACH5WoHL1Bn5UbpV2gGN0oDAABwZG538Br1V97SiK301Lc0AgAAgINzwYOXqctyR4PW1JPuaBAAAADH544HL1a75b7GPVVffUeDAAAAOAs3PXi9+i4fafRX9S0faTQAAAAn4rIHe1ED5jF95iONfkyfAQAA4HRc+WBfasZsq2cDAABwUi5+sEc1ZtbX8wAAADg11z/Ytfo0z9a3AwAAcA3ugXAMdW6+p+8CAADgYlwI4ZDq6Hyk0QAAAFyb+yEAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJycBhAAAADAyWkAAQAAAJza//t//x/sNAh3/72H4wAAAABJRU5ErkJggg==)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzZl2Ao0vBLP"
      },
      "source": [
        "---\n",
        "__2__ \n",
        "\n",
        "We decided that maybe we can use the number of characters and the average word length an essay to determine if the student should get an $A$ in a class or not.  Below are five samples of this data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RSC3cwLvBLP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "38de0a7e-d5bb-42ed-c471-eb06a7752035"
      },
      "source": [
        "theory_data2 = pd.DataFrame({\n",
        "    \"# of Chars\" : [216, 69, 302, 60, 393],\n",
        "    \"Average Word Length\" : [5.68, 4.78, 2.31, 3.16, 4.2],\n",
        "    \"Give an A\" : ['Yes', 'Yes', 'No', 'Yes', 'No']})\n",
        "theory_data2['bool'] = theory_data2['Give an A'].astype('category').cat.codes\n",
        "theory_data2"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># of Chars</th>\n",
              "      <th>Average Word Length</th>\n",
              "      <th>Give an A</th>\n",
              "      <th>bool</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>216</td>\n",
              "      <td>5.68</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>69</td>\n",
              "      <td>4.78</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>302</td>\n",
              "      <td>2.31</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60</td>\n",
              "      <td>3.16</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>393</td>\n",
              "      <td>4.20</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   # of Chars  Average Word Length Give an A  bool\n",
              "0         216                 5.68       Yes     1\n",
              "1          69                 4.78       Yes     1\n",
              "2         302                 2.31        No     0\n",
              "3          60                 3.16       Yes     1\n",
              "4         393                 4.20        No     0"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_XYyj_BvBLQ"
      },
      "source": [
        "a. What are the class priors, $P(A=Yes), P(A=No)$? (2pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlLWpFcLiGpk"
      },
      "source": [
        "P(A = Yes) = 3/5 = 0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKYjvevKiNcP"
      },
      "source": [
        "P(A = No) = 2/5 = 0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYvHDtQcvBLR"
      },
      "source": [
        "b. Find the parameters of the Gaussians necessary to do Gaussian Naive Bayes classification on this decision to give an A or not.  Standardize the features first over all the data together so that there is no unfair bias towards the features of different scales (2pts)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKhjYErMiUaB"
      },
      "source": [
        "Yes:\n",
        "*   Prior: 0.6\n",
        "*   Character #:\n",
        "  *   mean: -0.716\n",
        "  *   œÉ: 0.551\n",
        "*   Average Word Length:\n",
        "  *   mean: 0.434\n",
        "  *   œÉ: 0.879\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-G9jeW-jg5T"
      },
      "source": [
        "No:\n",
        "*   Prior: 0.4\n",
        "*   Character #:\n",
        "    *  mean: 1.074\n",
        "    *  œÉ: 0.551\n",
        "*   Average Word Length:\n",
        "    *  mean: -0.650\n",
        "    *  œÉ: 0.797\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5S21NR3vBLS"
      },
      "source": [
        "c. Using your response from the prior question, determine if an essay with 242 characters and an average word length of 4.56 should get an A or not (3pts)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVI34BUokQc1"
      },
      "source": [
        "Yes = 0.6 * 0.150 * 0.453 = 0.041"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G81khCohkgLU"
      },
      "source": [
        "No = 0.4 * 0.077 * 0.193 = 0.006"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkRRnd0aktIF"
      },
      "source": [
        "The essay should get an A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ3dEwwZvBLT"
      },
      "source": [
        "---\n",
        "__3__\n",
        "\n",
        "Consider the following questions pertaining to a k-Nearest Neighbors algorithm (1pt):\n",
        "\n",
        "a. How could you use a _validation set_ to determine the user-defined parameter $k$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLkjL0bok-GM"
      },
      "source": [
        "Start with k=1 then look for the k value that has the best precision and recall"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "egNLA2BOvBLT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQTy3LEYvBLU"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RKs9dL-vBLU"
      },
      "source": [
        "## 2 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjecEIaZvBLU"
      },
      "source": [
        "Let's train and test a _Logistic Regression Classifier_ to classify flowers from the Iris Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxyLMt8LvBLV"
      },
      "source": [
        "First download import the data from sklearn.datasets.  As mentioned in the Datasets area,  The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.  We will map this into a binary classification problem between Iris setosa versus Iris virgincia and versicolor.  We will use just the first 2 features, width and length of the sepals.  \n",
        "\n",
        "For this part, we will be practicing gradient descent with logistic regression.\n",
        "\n",
        "Use the following code to load the data, and binarize the target values.\n",
        "\n",
        "```\n",
        "iris = skdata.load_iris()\n",
        "X = iris.data[:, :2]\n",
        "y = (iris.target != 0) * 1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hsma4LFvBLV"
      },
      "source": [
        "__Write a script that:__\n",
        "\n",
        "\n",
        "1. Reads in the data with the script above.\n",
        "2. Standardizes the data using the mean and standard deviation\n",
        "3. Initialize the parameters of $\\theta$ using random values in the range [-1, 1]\n",
        "4. Do __batch__ gradient descent\n",
        "5.  Terminate when absolute value change in the loss on the data is less than $2^{-23}$, or after $10,000$ iterations have passed (whichever occurs first).\n",
        "6.  Use a learning rate $\\eta=0.01$.\n",
        "7. While the termination criteria (mentioned above in the implementation details) hasn't been met <br>\n",
        " a. Compute the loss of the data using the logistic regression cost<br>\n",
        " b. Update each parameter using __batch__ gradient descent<br>\n",
        " \n",
        "Plot the data and the decision boundary using matplotlib.  Verify your solution with the LogisticRegression sklearn method.\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lgr = LogisticRegression(penalty='none',solver='lbfgs',max_iter=10000)\n",
        "lgr.fit(X,y)\n",
        "```\n",
        "\n",
        "In your writeup, present the thetas from gradient descent that minimize the loss function as well as plots of your method versus the built in LogisticRegression method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFIRPSkHvBLW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCmBId_gdTYT"
      },
      "source": [
        "def standardize(data):\n",
        "  #get mean and std deviation\n",
        "  mean = np.mean(data, axis=0)\n",
        "  std = np.std(data, axis=0, ddof=1)\n",
        "  #standardize data\n",
        "  stddata = (data - mean)/std\n",
        "  return stddata\n",
        "\n",
        "def addBias(data):\n",
        "  #get the shape\n",
        "  m,n = np.shape(data)\n",
        "  #create a column of zeros size m\n",
        "  thetazero = np.ones((m, 1))\n",
        "  #add that to the data\n",
        "  XMat = np.hstack((thetazero, data))\n",
        "  return XMat\n",
        "\n",
        "def hypothesize(thetas, data):\n",
        "  #hypothesis for logistic regression\n",
        "  hyp = 1 / (1 + (np.power(np.e,-(data @ thetas))))\n",
        "  return hyp\n",
        "\n",
        "def accuracy(hyp, targety):\n",
        "  m = np.shape(hyp)\n",
        "  count = 0\n",
        "  for i in range(0, m[0]):\n",
        "    #count accuracy for all right predictions compared with test\n",
        "    if hyp[i] >= 0.5 and targety[i] >= 0.5 or hyp[i] < 0.5 and targety[i] < 0.5:\n",
        "      count += 1\n",
        "  acc = count / m[0]\n",
        "  return acc\n",
        "\n",
        "def recall(hyp, targety):\n",
        "  m = np.shape(hyp)\n",
        "  tp = 0\n",
        "  fn = 0\n",
        "  for i in range(0, m[0]):\n",
        "    #increment true positive\n",
        "    if hyp[i] >= 0.5 and targety[i] >= 0.5:\n",
        "      tp += 1\n",
        "    #increment false negative\n",
        "    elif hyp[i] < 0.5 and targety[i] >= 0.5:\n",
        "      fn+= 1\n",
        "  #calculate recall\n",
        "  rcl = tp / (tp+fn)\n",
        "  return rcl\n",
        "      \n",
        "def precision(hyp, targety):\n",
        "  m = np.shape(hyp)\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  for i in range(0, m[0]):\n",
        "    #increment true positive\n",
        "    if hyp[i] >= 0.5 and targety[i] >= 0.5:\n",
        "      tp += 1\n",
        "    #increment false positive\n",
        "    elif hyp[i] >= 0.5 and targety[i] < 0.5:\n",
        "      fp+= 1\n",
        "  prc = tp / (tp+fp)\n",
        "  return prc\n",
        "\n",
        "def f_measure(precision, recall):\n",
        "  return((2*precision*recall)/(precision+recall))\n",
        "\n",
        "  \n",
        "def RMSEF(y, y_hat):\n",
        "  return np.sqrt(np.sum((y - y_hat) ** 2) / len(y))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB2yAi4ovBLW"
      },
      "source": [
        "iris = load_iris()\n",
        "x = iris.data[:,:2]\n",
        "targety = (iris.target != 0) * 1\n",
        "x = standardize(x)\n",
        "XMat = addBias(x)\n",
        "m, n = np.shape(XMat)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMKJQR97wqWB"
      },
      "source": [
        "learningrate = 0.01\n",
        "iterations = 10000\n",
        "np.random.seed(0)\n",
        "thetas = np.random.randn(n) * np.sqrt(2. / n)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azQHc3oLcand"
      },
      "source": [
        "for i in range(iterations):\n",
        "  hyp = hypothesize(thetas, XMat)\n",
        "  loss =  targety - hyp\n",
        "  gradient = np.dot(XMat.T ,loss)\n",
        "  thetas +=  (learningrate/m) * gradient"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFA51tL2c192",
        "outputId": "afe452cf-b6b6-4694-fcf8-ca5b314a776b"
      },
      "source": [
        "lgr = LogisticRegression(penalty = 'none' , solver = 'lbfgs' , max_iter = 10000)\n",
        "lgr.fit(XMat, targety)\n",
        "skthetas = lgr.coef_\n",
        "skthetas"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 23.65086457,  71.20787484, -33.85885804]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgsPYySmdqbm"
      },
      "source": [
        "My plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "OlDvzHpJcdGy",
        "outputId": "491f8c8e-c177-4d8d-db99-5574e0d686dd"
      },
      "source": [
        "# Retrieve the model parameters.\n",
        "b = thetas[0]\n",
        "w1, w2 = thetas[1:].T\n",
        "# Calculate the intercept and gradient of the decision boundary.\n",
        "c = -b/w2\n",
        "m = -w1/w2\n",
        "\n",
        "# Plot the data and the classification with the decision boundary.\n",
        "xmin, xmax = -1, 4\n",
        "ymin, ymax = -1, 4.5\n",
        "xd = np.array([xmin, xmax])\n",
        "yd = m*xd + c\n",
        "plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
        "plt.fill_between(xd, yd, ymin, color='tab:green', alpha=0.2)\n",
        "plt.fill_between(xd, yd, ymax, color='tab:red', alpha=0.2)\n",
        "\n",
        "plt.scatter(*x[targety==0].T, s=8, alpha=0.5)\n",
        "plt.scatter(*x[targety==1].T, s=8, alpha=0.5)\n",
        "plt.xlim(xmin, xmax)\n",
        "plt.ylim(ymin, ymax)\n",
        "plt.ylabel(r'$x_2$')\n",
        "plt.xlabel(r'$x_1$')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt1t5W8qdvXq"
      },
      "source": [
        "sklearn plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "mJUYGLeKcc6B",
        "outputId": "771f0f3c-029e-4a55-dff1-425685e0918a"
      },
      "source": [
        "# Retrieve the model parameters.\n",
        "b = skthetas[0][0]\n",
        "w1, w2 = skthetas[0][1:].T\n",
        "# Calculate the intercept and gradient of the decision boundary.\n",
        "c = -b/w2\n",
        "m = -w1/w2\n",
        "\n",
        "# Plot the data and the classification with the decision boundary.\n",
        "xmin, xmax = -1, 4\n",
        "ymin, ymax = -1, 4.5\n",
        "xd = np.array([xmin, xmax])\n",
        "yd = m*xd + c\n",
        "plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
        "plt.fill_between(xd, yd, ymin, color='tab:green', alpha=0.2)\n",
        "plt.fill_between(xd, yd, ymax, color='tab:red', alpha=0.2)\n",
        "\n",
        "plt.scatter(*x[targety==0].T, s=8, alpha=0.5)\n",
        "plt.scatter(*x[targety==1].T, s=8, alpha=0.5)\n",
        "plt.xlim(xmin, xmax)\n",
        "plt.ylim(ymin, ymax)\n",
        "plt.ylabel(r'$x_2$')\n",
        "plt.xlabel(r'$x_1$')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8MZs8dUvBLX"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H8p3MpOvBLX"
      },
      "source": [
        "## 3 Logistic Regression Spam Classification "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gmJi_MBvBLX"
      },
      "source": [
        "Let's train and test a _Logistic Regression Classifier_ to classifiy Spam or Not from the Spambase Dataset.\n",
        "\n",
        "First download the dataset _spambase.data_ from Blackboard.  As mentioned in the Datasets area, this dataset contains 4601 rows of data, each with 57 continuous valued features followed by a binary class label (0=not-spam, 1=spam).  There is no header information in this file and the data is comma separated.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puTpk-JfvBLY"
      },
      "source": [
        "__Write a script that:__\n",
        "\n",
        "\n",
        "1. Reads in the data.\n",
        "2. Randomizes the data.\n",
        "3. Selects the first 2/3 (round up) of the data for training and the remaining for testing (you may use  __sklearn train\\_test\\_split__ for this part)\n",
        "4. Standardizes the data (except for the last column of course) using the training data\n",
        "5. Initialize the parameters of $\\theta$ using random values in the range [-1, 1]\n",
        "6.  Do _batch gradient descent_\n",
        "7. Terminate when absolute value change in the loss on the data is less than $2^{-23}$, or after $1,500$ iterations have passed (whichever occurs first, this will likely be a slow process).\n",
        "8. Use a learning rate $\\eta=0.01$.\n",
        "9. Classify each testing sample using the model and choosing the class label based on which class probability is higher.\n",
        "10. Computes the following statistics using the testing data results:<br>\n",
        "[lecture 3 b] <br>\n",
        " a. Precision<br>\n",
        " b. Recall<br>\n",
        " c. F-measure<br>\n",
        " d. Accuracy<br>\n",
        " \n",
        "__Implementation Details__\n",
        "\n",
        "1. Seed the random number generate with zero prior to randomizing the data\n",
        "2. There are a lot of $\\theta$s and this will likely be a slow process\n",
        "\n",
        "__In your report you will need__\n",
        "\n",
        "1. The statistics requested for your Logistic classifier run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS_8QGVavBLY"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/gitparrot/MLHW2/main/spambase.data'\n",
        "csvdata = pd.read_csv(url, header=None)\n",
        "\n",
        "csvdata.iloc[:,:-1] -= np.mean(csvdata.iloc[:,:-1])\n",
        "csvdata.iloc[:,:-1] /= np.std(csvdata.iloc[:,:-1])\n",
        "\n",
        "# Splitting into training and test data\n",
        "test_data = csvdata.sample(frac=0.33, random_state=0)\n",
        "train_data = csvdata.drop(test_data.index)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69WMur7Xd60y"
      },
      "source": [
        "m, n = train_data.shape"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3dKmvRMd8zo"
      },
      "source": [
        "testdata = test_data.iloc[:,:-1].values\n",
        "testY = test_data.iloc[:,-1].values\n",
        "traindata = train_data.iloc[:,:-1].values\n",
        "trainY = train_data.iloc[:,-1].values"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfedR76Qd-f-"
      },
      "source": [
        "XMat = addBias(traindata)\n",
        "testdata = addBias(testdata)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39VVg-leeB7F"
      },
      "source": [
        "learningrate = 0.01\n",
        "iterations = 1500\n",
        "np.random.seed(0)\n",
        "thetas = np.random.randn(n) * np.sqrt(2. / n)\n",
        "js = []"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1jam7nTeD-x"
      },
      "source": [
        "for i in range(iterations):\n",
        "  hyp = hypothesize(thetas, XMat)\n",
        "  loss =  trainY - hyp\n",
        "  js.append(np.sum(trainY * ln(hyp + (1 - trainY)) * ln(1 - hyp)) / trainY.shape[0])\n",
        "  gradient = np.dot(XMat.T,loss)\n",
        "  thetas +=  (learningrate/m) * gradient\n",
        "  if i > 1 and js[-2] - js[-1] < 2 ** -23:\n",
        "    break"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy6WcNvweGKd"
      },
      "source": [
        "pred = hypothesize(thetas, testdata)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZoyWGlPeIdx",
        "outputId": "3d06cd34-6cc4-4d1f-b1cc-ec537e60370b"
      },
      "source": [
        "myacc = accuracy(pred, testY)\n",
        "myrcl = recall(pred, testY)\n",
        "myprc = precision(pred, testY)\n",
        "myfms = f_measure(myprc,myrcl)\n",
        "print('Accuracy: ',myacc)\n",
        "print('Recall: ',myrcl)\n",
        "print('Precision: ',myprc)\n",
        "print('F-Measure: ',myfms)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9084321475625824\n",
            "Recall:  0.8552845528455284\n",
            "Precision:  0.9131944444444444\n",
            "F-Measure:  0.8832913518052057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8qvPGOOvBLY"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuS47IjQvBLZ"
      },
      "source": [
        "## 4 Naive Bayes Classifier\n",
        "\n",
        "Let's train and test a _Naive Bayes Classifier_ to classifiy Spam or Not from the Spambase Dataset.\n",
        "\n",
        "First download the dataset _spambase.data_ from Blackboard.  As mentioned in the Datasets area, this dataset contains 4601 rows of data, each with 57 continuous valued features followed by a binary class label (0=not-spam, 1=spam).  There is no header information in this file and the data is comma separated.  As always, your code should work on any dataset that lacks header information and has several comma-separated continuous-valued features followed by a class id $\\in {0,1}$.\n",
        "\n",
        "__Write a script that:__\n",
        "\n",
        "1. Reads in the data.\n",
        "2. Randomizes the data.\n",
        "3. Selects the first 2/3 (round up) of the data for training and the remaining for testing\n",
        "4. Standardizes the data (except for the last column of course) using the training data\n",
        "5. Divides the training data into two groups: Spam samples, Non-Spam samples.\n",
        "6. Creates Gaussian models for each feature for each class.\n",
        "7. Classify each testing sample using these models and choosing the class label based on which class probability is higher.\n",
        "8. Computes the following statistics using the testing data results:<br>\n",
        " a. Precision<br>\n",
        " b. Recall<br>\n",
        " c. F-measure<br>\n",
        " d. Accuracy<br>\n",
        " \n",
        "__Implementation Details__\n",
        "\n",
        "1.  Seed the random number generate with zero prior to randomizing the data\n",
        "2.  If  you  decide  to  work  in  log  space,  realize  that  python  interprets  0log0  as  inf. You  should identify this situation and either add an EPS (very small positive number) or add a very largenegative number to the log sum.\n",
        "\n",
        "__In your report you will need:__\n",
        "\n",
        "1.  The statistics requested for your Naive Bayes classifier run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a4k09F6dEcv"
      },
      "source": [
        "def class_seperator(data):\n",
        "  #create a dictionary that is partitioned on class\n",
        "\tpartition = dict()\n",
        "  #go through data keep track of row\n",
        "\tfor i in range(len(data)):\n",
        "\t\trow = data[i]\n",
        "    #get the class id\n",
        "\t\tclass_id = row[-1]\n",
        "    #if we come across a new class create a new list in the dictionary with the class id as key\n",
        "\t\tif (class_id not in partition):\n",
        "\t\t\tpartition[class_id] = list()\n",
        "    #append the row contents to the dictionary list under the class id\n",
        "\t\tpartition[class_id].append(row)\n",
        "\treturn partition\n",
        "\n",
        "def summarize(data):\n",
        "  #store the mean std deviation and len of the dataset\n",
        "\tsummaries = [(np.mean(column), np.std(column), len(column)) for column in zip(*data)]\n",
        "\tdel(summaries[-1])\n",
        "\treturn summaries\n",
        "\n",
        "def class_summarizer(dataset):\n",
        "  #uses class separator function to create a dictionary\n",
        "\tpartitioned = class_seperator(dataset)\n",
        "  #creates a dictionary to store summaries by class\n",
        "\tsummary = dict()\n",
        "\tfor classid, rows in partitioned.items():\n",
        "    #store the summary in a summary dictionary\n",
        "\t\tsummary[classid] = summarize(rows)\n",
        "\treturn summary\n",
        "\n",
        "def gpdf(x, mean, std):\n",
        "  #create small number to prevent division by zero\n",
        "  SMALL = np.finfo(float).eps\n",
        "  #add that to mean and std\n",
        "  mean += SMALL\n",
        "  std += SMALL\n",
        "  #calculate gaussian distribution\n",
        "  exponent = exp(-((x-mean)**2 / (2 * std**2 )))\n",
        "  return (1 / (sqrt(2 * pi) * std)) * exponent\n",
        "\n",
        "def probability_func(summ, row):\n",
        "  #get total rows. the summ[output][0][2] gives you the number of data for the class\n",
        "\ttotal_rows = sum([summ[output][0][2] for output in summ])\n",
        "  #create dictionary to store probabilities\n",
        "\tprobabilities = dict()\n",
        "\tfor classid, csumm in summ.items():\n",
        "    # number of rows in class divided by rows in total\n",
        "\t\tprobabilities[classid] = summ[classid][0][2]/float(total_rows)\n",
        "    #get mean and std from summary dict and plug them into gpdf function\n",
        "\t\tfor i in range(len(csumm)):\n",
        "\t\t\tmean, stdev, _ = csumm[i]\n",
        "\t\t\tprobabilities[classid] *= gpdf(row[i], mean, stdev)\n",
        "\treturn probabilities\n",
        "\n",
        "def predict(summaries, row):\n",
        "  #get probabilites from function\n",
        "\tprobs = probability_func(summaries, row)\n",
        "  #initizialize the best label and best probability\n",
        "\tguess, chance = None, -1\n",
        "  #update chance and guess\n",
        "\tfor classid, probability in probs.items():\n",
        "\t\tif guess is None or probability > chance:\n",
        "\t\t\tchance = probability\n",
        "\t\t\tguess = classid\n",
        "\treturn guess\n",
        "\n",
        "def nb(train, test):\n",
        "  #create summary dictionary\n",
        "\tsummarize = class_summarizer(train)\n",
        "  #create a list to hold predictions\n",
        "\tpredictions = list()\n",
        "\tfor row in test:\n",
        "    #make prediction based off row and add to list\n",
        "\t\toutput = predict(summarize, row)\n",
        "\t\tpredictions.append(output)\n",
        "\treturn(predictions)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vciWKgUSvBLZ"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/gitparrot/MLHW2/main/spambase.data'\n",
        "csvdata = pd.read_csv(url, header=None, index_col=False)\n",
        "csvdata = csvdata.sample(frac=1, random_state=0)\n",
        "csvdata.iloc[:,:-1] -= np.mean(csvdata.iloc[:,:-1])\n",
        "csvdata.iloc[:,:-1] /= np.std(csvdata.iloc[:,:-1])\n",
        "csvdata = csvdata.to_numpy()\n",
        "csvdata = csvdata.astype(np.float)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02_EPLyLeOdf"
      },
      "source": [
        "m, n = csvdata.shape"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6LVpJNfeQyu"
      },
      "source": [
        "trainrows = int((2/3)*m)\n",
        "testrows = m - trainrows"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQaF3PP6eQro"
      },
      "source": [
        "testdata = csvdata[trainrows:]\n",
        "traindata = csvdata[:trainrows]\n",
        "testY = testdata[:,-1]\n",
        "trainY = traindata[:,-1]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnwIFkWFgi8y"
      },
      "source": [
        "pred = nb(traindata, testdata)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewwj6BE3eQnP",
        "outputId": "93e00d4f-027a-4f31-a9af-adee33e1fcd7"
      },
      "source": [
        "myacc = accuracy(pred, testY)\n",
        "myrcl = recall(pred, testY)\n",
        "myprc = precision(pred, testY)\n",
        "myfms = f_measure(myprc,myrcl)\n",
        "print('Accuracy: ',myacc)\n",
        "print('Recall: ',myrcl)\n",
        "print('Precision: ',myprc)\n",
        "print('F-Measure: ',myfms)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.7698826597131682\n",
            "Recall:  0.9791666666666666\n",
            "Precision:  0.6232044198895028\n",
            "F-Measure:  0.7616475354490209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCZuzAksvBLZ"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uc-yS9OvBLa"
      },
      "source": [
        "## 5  Decision Trees\n",
        "\n",
        "Let‚Äôs train and test a Decision Tree to classify Spam or Not from the Spambase Dataset.\n",
        "\n",
        "__Write a script that:__\n",
        "\n",
        "1. Reads in the data.\n",
        "2. Randomizes the data.\n",
        "3. Selects the first 2/3 (round up) of the data for training and the remaining for testing\n",
        "4. Standardizes the data (except for the last column of course) using the training data\n",
        "5. Divides the training data into two groups:  Spam samples, Non-Spam samples.\n",
        "6. Trains a decision tree using the ID3 algorithm without any pruning.\n",
        "7. Classify each testing sample using your trained decision tree.\n",
        "8. Computes the following statistics using the testing data results:<br>\n",
        " a.  Precision <br>\n",
        " b.  Recall<br>\n",
        " c.  F-measure<br>\n",
        " d.  Accuracy<br>\n",
        " \n",
        "__Implementation Details__\n",
        "\n",
        "1.  Seed the random number generate with zero prior to randomizing the data\n",
        "2.  Depending on your perspective, the features are either continuous or finite discretize.  The lattercan be considered tru since the real-values are just the number of times a feature is observedin an email, normalized by some other count.  That being said, for a decision tree we normallyuse categorical or discretized features. So for the purpose of this dataset, look at therange of each feature and turn them into binary features by choosing a threshold. I suggest using the median or mean.\n",
        "\n",
        "__In your report you will need:__ \n",
        "1.  The statistics requested for your Decision Tree classifier run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unjbLyN_vBLa"
      },
      "source": [
        "#### Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isFWhF1GvBLb"
      },
      "source": [
        "_Notes hand typed from Wikipedia; nearly verbatim_\n",
        "\n",
        "__Algorithm__\n",
        "\n",
        "The ID3 - iterative dichotomiser 3 - is used to generate a decision tree from a dataset. ID3 is the precusor to C4.5 and is typically used in machine learning and natural language processing domains.\n",
        "\n",
        "The ID3 algorithm begins witht the origianl set S as the root node. On each iteration of the algorithm, it iterates through every unused attribute of the set S and calculates the entropy H(Y) or the information gain IG(S) of that attribute. \n",
        "\n",
        "It then selects the attribute which has the smallest entropy (or largest information gain) value. The set S is then split or partitioned be the selected attribute to produce subsets of the data. For example, a node can be split into child nodes based on the subsets of the population whose ages are less than 50, between 50 and 100, and greater than 100). The algorithm continues to recurse on each subset, considering only attributes never slected before.\n",
        "\n",
        "Recursion on a subset may stop in one of these cases:\n",
        "\n",
        "- every element in the subset belongs to the same class; in which case the node is turned into a leaf node and labelled with the class of the examples.\n",
        "- there are no more attributes to be selected, but the examples still do not belong to the same class. In this case, the node is made a leaf node and lablled with the most common class of the examples in the subset.\n",
        "- ther are no examples in the subset, which happens when no example in the parent set was found to match a specific value of the selected attribute. An example could be the absence of a person among the population with age over 100 years. The leaf node is created and labelled with the most common class of the examples in the parent nodes set.\n",
        "\n",
        "Throughout the algorithm, the decision tree is constructed with each non-terminal node (internal node) representing the selected attribute on which the data was split, and terminal nodes (leaf nodes) representing the class label of the final subset of this branch.\n",
        "\n",
        "\n",
        "__Summary__\n",
        "1. Calculate the entropy of every attribute $\\alpha$ of $S$.\n",
        "2. Partition (split) the set $S$ into subsets using the attribute for which the resulting entropy after splitting is minimized; or equivalenty, information gain is maximum.\n",
        "3. Make a decision tree node containing that attribute.\n",
        "4. Recurse on subsets using the remaining attributes.\n",
        "\n",
        "\n",
        "__Pseudocode__\n",
        "\n",
        "```\n",
        "ID3 (Examples, Target_Attribute, Attributes)\n",
        "    \n",
        "    Create a root node for the tree\n",
        "    \n",
        "    If all examples are positive: Return the single-node tree Root with label = +\n",
        "    If all examples are negative: Return the single-node tree Root with label = -\n",
        "    If number of predicting attributes is empty: Return the single-node tree Root\n",
        "    \n",
        "    If Root not None: label = most common value of the target attribute in the example\n",
        "    \n",
        "    Else If Root is None:\n",
        "            \n",
        "            A = the attribute the best classifies the examples\n",
        "            Decision Tree attribute for Root = A\n",
        "            \n",
        "            For value $v_i$ in A:\n",
        "                Add a new tree branch below Root, correspongin to the test A = $v_i$\n",
        "                Let Examples($v_i$) be the subset of examples that have the value $v_i$ for A\n",
        "                If Examples($v_i$) is empty:\n",
        "                    Then below this new branch add a leaf node with label = most common target value in the examples\n",
        "                Else below this new branch add the subtree ID3(Examples($v_i$), Target_attribute, Attributes - {A})\n",
        "    \n",
        "    End\n",
        "    Return Root\n",
        "```\n",
        "\n",
        "__Properties__\n",
        "ID3 does not guarantee an optimal solution. It can converge on local optima. It uses a greedy strategy by selecting the locally best attribute to split the dataset on each iteration. The algorithms optimality can be improved by using backtracking during the search for the optimal decision tree at the cost of possibly taking longer.\n",
        "\n",
        "ID3 can overfit the training data. To avoid overfitting, smaller decision trees should be preferred over larger ones. This algorithm usually produces small trees but it does not always produce the smallest tree possible.\n",
        "\n",
        "ID3 is harder to use on continuous data than on factored data (factored data has a discrete number of possible values, thus reducing the possible barnch points). If the values of any given attribute are continuous, then there are many more places to split the data on this attribute, and searching for the best value to split on can be time consuming. \n",
        "\n",
        "__Usage__\n",
        "ID3 is used by training on data set $S$ to produce a decision tree which is stored in memory. At runtime, this decision tree is used to classify new test cases (feature vectors) by traversing the decision tree using the features of the datum to arrive at a leaf node. The class of this terminal node is the class the test case is classified as."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYv22_0ldlzy"
      },
      "source": [
        "#Node needed to build tree\n",
        "class Node:\n",
        "    def __init__(self, label):\n",
        " \n",
        "        self.feature = None  \n",
        "        self.feature_values = []  \n",
        "        self.label = label   \n",
        "        self.children = {}   \n",
        "        self.parent_feature = None\n",
        "        self.parent_feature_value = None\n",
        "        self.change = False\n",
        "        self.instances_labeled = []\n",
        " \n",
        "# Find most common class\n",
        "def popular_class(instances):\n",
        "    labels = []  \n",
        "    for instance in instances:\n",
        "        labels.append(instance[57])\n",
        "\n",
        "    return Counter(labels).most_common(1)[0][0]\n",
        "\n",
        "# Calculate prior entropy\n",
        "def p_ent(instances):   \n",
        "    labels = []  \n",
        "    for instance in instances:\n",
        "        labels.append(instance[57])\n",
        "    counter = Counter(labels)\n",
        "    if len(counter) == 1:\n",
        "        return 0\n",
        "    else:\n",
        "        entropy = 0\n",
        "        for c, count_of_c in counter.items():\n",
        "            probability = count_of_c / len(labels)\n",
        "            entropy += probability * (math.log(probability, 2))\n",
        "        return -entropy\n",
        "\n",
        "# Calculate entropy\n",
        "def entropy(instances, feature, feature_value):\n",
        "\n",
        "    labels = []\n",
        " \n",
        "    for instance in instances:\n",
        "        if instance[feature] == feature_value:\n",
        "            labels.append(instance[57])\n",
        "    counter = Counter(labels)\n",
        "    \n",
        "    if len(counter) == 1:\n",
        "        return 0\n",
        "    else:\n",
        "        entropy = 0\n",
        "        for c, count_of_c in counter.items():\n",
        "            probability = count_of_c / len(labels)\n",
        "            entropy += probability * (math.log(probability, 2))\n",
        "        return -entropy\n",
        "\n",
        "# Calculate IG\n",
        "def information_gain(instances, feature):\n",
        "    #prior entropy\n",
        "    pentropy = p_ent(instances)\n",
        " \n",
        "    values = []\n",
        "    #append features to a list\n",
        "    for instance in instances:\n",
        "        values.append(instance[feature])\n",
        "    counter = Counter(values) \n",
        "    r_ent = 0\n",
        "    s_info = 0\n",
        " \n",
        "    for feature_value, feature_value_count in counter.items():\n",
        "        probability = feature_value_count/len(values)\n",
        "        #remaining entropy\n",
        "        r_ent += (probability * entropy(\n",
        "            instances, feature, feature_value))\n",
        "        #split information\n",
        "        s_info += probability * (math.log(probability, 2))\n",
        "    information_gain = pentropy - r_ent\n",
        "    s_info = -s_info\n",
        " \n",
        "    gainratio = None\n",
        " \n",
        "    if s_info != 0:\n",
        "        gainratio = information_gain / s_info\n",
        "    else:\n",
        "        gainratio = -1000\n",
        " \n",
        "    return gainratio\n",
        "\n",
        "# Build tree using recursion    \n",
        "def ID3(instances, features, default):\n",
        "\n",
        "    if len(instances) == 0:\n",
        "        return Node(default)\n",
        " \n",
        "    labels = []  \n",
        "    for instance in instances:\n",
        "        labels.append(instance[57])\n",
        "        \n",
        "    # Check if we only have one class\n",
        "    if len(Counter(labels)) == 1 or len(labels) == 1:\n",
        "        tree = Node(popular_class(instances))\n",
        "        return tree\n",
        "    #find best feature\n",
        "    else:\n",
        "        #traverse tree and assign best feature to a node\n",
        "        best_feature = find_best_feature(instances, features)\n",
        "        tree = Node(popular_class(instances))\n",
        "        tree.feature = best_feature\n",
        "        best_feature_values = []\n",
        "        for instance in instances:\n",
        "            try:\n",
        "                best_feature_values.append(instance[best_feature])\n",
        "            except:\n",
        "                no_best_feature = True\n",
        "        tree.feature_values = list(set(best_feature_values))\n",
        "        \n",
        "        # loop through each value of the best feature (0, 1)\n",
        "        for best_feat_value_i in tree.feature_values:\n",
        "            instances_i = []\n",
        "            for instance in instances:\n",
        "                if instance[best_feature] == best_feat_value_i:\n",
        "                    instances_i.append(instance)\n",
        "            subtree = ID3(instances_i, features, popular_class(instances))\n",
        "            subtree.instances_labeled = instances_i\n",
        "            subtree.parent_feature = best_feature \n",
        "            subtree.parent_feature_value = best_feat_value_i \n",
        "            tree.children[best_feat_value_i] = subtree\n",
        " \n",
        "        return tree\n",
        " \n",
        "# Find the best feature to split on\n",
        "def find_best_feature(instances, features):\n",
        "    selected_feature = None\n",
        "    max_IG = -1000\n",
        "    #update information gain\n",
        "    for feature in features:\n",
        "        gain = information_gain(instances, feature)\n",
        "        #keep track of max gain and take note of the feature associated\n",
        "        if gain > max_IG:\n",
        "            max_IG = gain\n",
        "            selected_feature = feature\n",
        " \n",
        "    return selected_feature\n",
        " \n",
        "def predict_list(trained_tree, test_instances):\n",
        "    pred_list = []\n",
        "    # loop through each example and find the predication for it\n",
        "    for test_instance in test_instances:\n",
        "        pred_list.append([predict(trained_tree, test_instance), test_instance[57]])  \n",
        "    return pred_list\n",
        "\n",
        " \n",
        "def predict(node, test_instance):\n",
        "    #basecase\n",
        "    if len(node.children) == 0:\n",
        "        return node.label\n",
        "    #recurse\n",
        "    else:\n",
        "        feature_value = test_instance[node.feature]\n",
        "        if feature_value in node.children and node.children[\n",
        "            feature_value].change == False:\n",
        "            return predict(node.children[feature_value], test_instance)\n",
        "        else:\n",
        "            instances = []\n",
        "            for attr_value in node.feature_values:\n",
        "                instances += node.children[attr_value].instances_labeled\n",
        "            return popular_class(instances)\n",
        "        "
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsn_Bo0Eq--7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/gitparrot/MLHW2/main/spambase.data'\n",
        "csvdata = pd.read_csv(url, header=None)\n",
        "\n",
        "csvdata.iloc[:,:-1] -= np.mean(csvdata.iloc[:,:-1])\n",
        "csvdata.iloc[:,:-1] /= np.std(csvdata.iloc[:,:-1])\n",
        "\n",
        "# Splitting into training and test data\n",
        "test_data = csvdata.sample(frac=0.33, random_state=0)\n",
        "train_data = csvdata.drop(test_data.index)\n",
        "\n",
        "mean = np.mean(train_data.iloc[:,:-1])\n",
        "\n",
        "train_data[train_data < mean] = 0\n",
        "train_data[train_data >= mean] = 1\n",
        "train_data = train_data.astype(np.int32)\n",
        "\n",
        "test_data[test_data < mean] = 0\n",
        "test_data[test_data >= mean] = 1\n",
        "test_data = test_data.astype(np.int32)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5EC-EoAnxpe"
      },
      "source": [
        "default = popular_class(train_data.values)\n",
        "tree = ID3(train_data.values.tolist(), train_data.iloc[:,:-1].columns.values.tolist(), default)\n",
        "pred_list = predict_list(tree, test_data.values)  "
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4vX-Nujo7L8",
        "outputId": "062e0abf-f4b9-4553-89e2-32d28a5b6332"
      },
      "source": [
        "pre_array = np.array(pred_list)\n",
        "myacc = accuracy(pre_array[:,0], pre_array[:,1])\n",
        "myrcl = recall(pre_array[:,0], pre_array[:,1])\n",
        "myprc = precision(pre_array[:,0], pre_array[:,1])\n",
        "myfms = f_measure(myprc, myrcl)\n",
        "print('Accuracy: ',myacc)\n",
        "print('Recall: ', myrcl)\n",
        "print('Precision: ', myprc)\n",
        "print('F-Measure: ', myfms)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8814229249011858\n",
            "Recall:  0.8390243902439024\n",
            "Precision:  0.864321608040201\n",
            "F-Measure:  0.8514851485148514\n"
          ]
        }
      ]
    }
  ]
}